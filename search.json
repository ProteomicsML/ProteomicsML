[
  {
    "objectID": "datasets/fragmentation/index.html",
    "href": "datasets/fragmentation/index.html",
    "title": "Fragmentation",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/fragmentation/NIST.html",
    "href": "datasets/fragmentation/NIST.html",
    "title": "NIST Peptide libraries",
    "section": "",
    "text": "Attributes\n\ndata type: Fragmentation intensity\ntitle: NIST\ntag: NIST\ndata publication: https://chemdata.nist.gov/dokuwiki/lib/exe/fetch.php?media=peptidew:sergey_sheetlin_asms2020.pdf\nML publication:\nsource dataset identifier:\nspecies: Homo sapiens (human)\nsize: 646 MB (gzipped)\nformat: MSP\ncolumns:\nmass modifications: unmodified & oxidation\nchromatography_column_type: \n\n\n\nData description\nConsensus spectral libraries generated by NIST, the US National Institute of Standards and Technology.\n\n\nComments\n\nData on chemdata.nist.gov\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/fragmentation/ProteomeTools_FI.html",
    "href": "datasets/fragmentation/ProteomeTools_FI.html",
    "title": "ProteomeTools",
    "section": "",
    "text": "Attributes\n\ndata type: Fragmentation intensity\ntitle: ProteomeTools synthetic peptides\ntag: ProteomeTools\ndata publication: https://doi.org/10.1038/nmeth.4153\nML publication: https://doi.org/10.1038/s41592-019-0426-7\nsource dataset identifier: PXD004732\nspecies: Homo sapiens (human)\nsize: Train (4.87 GB). Holdout (250 MB)\nformat: hdf5\ncolumns:\nmass modifications: unmodified & oxidation\nchromatography_column_type: \n\n\n\nData description\nThe ProteomeTools project aims to derive molecular and digital tools from the human proteome to facilitate biomedical and life science research. Here, we describe the generation and multimodal LC-MS/MS analysis of >350,000 synthetic tryptic peptides representing nearly all canonical human gene products. This resource will be extended to 1.4 million peptides within two years and all data will be made available to the public in ProteomicsDB.\n\n\nSample protocol description\nTryptic peptides were individually synthesized by solid phase synthesis, combined into pools of ~1,000 peptides and measured on an Orbitrap Fusion mass spectrometer. For each peptide pool, an inclusion list was generated to target peptides for fragmentation in further LC-MS experiments using five fragmentation methods (HCD, CID, ETD, EThCD, ETciD) with ion trap or Orbitrap readout and HCD spectra were recorded at 6 different collision energies.\n\n\nData analysis protocol\nLC-MS runs were individually analyzed using MaxQuant 1.5.3.30.\n\n\nComments\n\nData on FigShare\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/index.html",
    "href": "datasets/index.html",
    "title": "Datasets",
    "section": "",
    "text": "[TODO: Add dataset introduction]\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/index.html#fragmentation",
    "href": "datasets/index.html#fragmentation",
    "title": "Datasets",
    "section": "Fragmentation",
    "text": "Fragmentation\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nNIST Peptide libraries\n\n\nSep 10, 2022\n\n\n\n\nProteomeTools\n\n\nSep 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html#ion-mobility",
    "href": "datasets/index.html#ion-mobility",
    "title": "Datasets",
    "section": "Ion mobility",
    "text": "Ion mobility\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nMeier et al.¬†TIMS\n\n\nSep 10, 2022\n\n\n\n\nVan Puyvelde et al.¬†TWIMS\n\n\nSep 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/index.html#retention-time",
    "href": "datasets/index.html#retention-time",
    "title": "Datasets",
    "section": "Retention time",
    "text": "Retention time\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nDLOmix\n\n\nSep 10, 2022\n\n\n\n\nProteomeTools\n\n\nSep 10, 2022\n\n\n\n\nSharma et al.¬†HeLa\n\n\nSep 10, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "datasets/ionmobility/index.html",
    "href": "datasets/ionmobility/index.html",
    "title": "Ion mobility",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/ionmobility/Meier_TIMS.html",
    "href": "datasets/ionmobility/Meier_TIMS.html",
    "title": "Meier et al.¬†TIMS",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide ion mobility\ntitle: Deep learning the collisional cross sections of the peptide universe from a million experimental values\ntag: Meier_IM_CCS\ndata publication: https://doi.org/10.1074/mcp.tir118.000900\nML publication: https://doi.org/10.1038/s41467-021-21352-8\nsource dataset identifier: PXD010012, PXD019086, PXD017703\nspecies: Homo sapiens (Human), Saccharomyces cerevisiae (Baker‚Äôs yeast)\nsize: 718.917 (large)\nformat: CSV\ncolumns: index, Modified sequence, Charge, Mass, Intensity, Retention time, CCS, PT\nmass modifications: unmodified & oxidation & acetylation & carbamidomethyl\nionmobility_type: TIMS\n\n\n\nData description\nMS raw files were analyzed with MaxQuant version 1.6.5.0, which extracts 4D isotope patterns (‚Äòfeatures‚Äô) and associated MS/MS spectra. The built-in search engine Andromeda74 was used to match observed fragment ions to theoretical peptide fragment ion masses derived from in silico digests of a reference proteome and a list of 245 potential contaminants using the appropriate digestion rules for each proteolytic enzyme (trypsin, LysC or LysN). We allowed a maximum of two missing values and required a minimum sequence length of 7 amino acids while limiting the maximum peptide mass to 4600‚ÄâDa. Carbamidomethylation of cysteine was defined as a fixed modification, and oxidation of methionine and acetylation of protein N-termini were included in the search as variable modifications. Reference proteomes for each organism including isoforms were accessed from UniProt (Homo sapiens: 91,618 entries, 2019/05; E. coli: 4403 entries, 2019/01; C. elegans: 28,403 entries, 2019/01; S. cerevisiae: 6049 entries, 2019/01; D. melanogaster: 23,304 entries, 2019/01). The synthetic peptide library (ProteomeTools54) was searched against the entire human reference proteome. The maximum mass tolerances were set to 20 and 40‚Äâppm for precursor and fragment ions, respectively. False discovery rates were controlled at 1% on both the peptide spectrum match and protein level with a target-decoy approach. The analyses were performed separately for each organism and each set of synthetic peptides (‚Äòproteotypic set‚Äô, ‚ÄòSRM atlas‚Äô, and ‚Äòmissing gene set‚Äô). To demonstrate the utility of CCS prediction, we re-analyzed three diaPASEF experiments from Meier et al.55 with Spectronaut 14.7.201007.47784 (Biognosys AG), replacing experimental ion mobility values in the spectral library with our predictions. Singly charged peptide precursors were excluded from this analysis as the neural network was exclusively trained with multiply charged peptides.\n\n\nSample protocol description\nIn bottom-up proteomics, peptides are separated by liquid chromatography with elution peak widths in the range of seconds, while mass spectra are acquired in about 100 microseconds with time-of-fight (TOF) instruments. This allows adding ion mobility as a third dimension of separation. Among several formats, trapped ion mobility spectrometry (TIMS) is attractive due to its small size, low voltage requirements and high efficiency of ion utilization. We have recently demonstrated a scan mode termed parallel accumulation ‚Äì serial fragmentation (PASEF), which multiplies the sequencing speed without any loss in sensitivity (Meier et al., PMID: 26538118). Here we introduce the timsTOF Pro instrument, which optimally implements online PASEF. It features an orthogonal ion path into the ion mobility device, limiting the amount of debris entering the instrument and making it very robust in daily operation. We investigate different precursor selection schemes for shotgun proteomics to optimally allocate in excess of 100 fragmentation events per second. More than 800,000 fragmentation spectra in standard 120 min LC runs are easily achievable, which can be used for near exhaustive precursor selection in complex mixtures or re-sequencing weak precursors. MaxQuant identified more than 6,000 proteins in single run HeLa analyses without matching to a library, and with high quantitative reproducibility (R > 0.97). Online PASEF achieves a remarkable sensitivity with more than 2,000 proteins identified in 30 min runs of only 10 ng HeLa digest. We also show that highly reproducible collisional cross sections can be acquired on a large scale (R > 0.99). PASEF on the timsTOF Pro is a valuable addition to the technological toolbox in proteomics, with a number of unique operating modes that are only beginning to be explored.\n\n\nData analysis protocol\nSee Data description\n\n\nComments\n/\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/ionmobility/VanPuyvelde_TWIMS.html",
    "href": "datasets/ionmobility/VanPuyvelde_TWIMS.html",
    "title": "Van Puyvelde et al.¬†TWIMS",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide ion mobility\ntitle: A comprehensive LFQ benchmark dataset on modern day acquisition strategies in proteomics\ntag: VanPuyvelde_TWIMS_CCS\ndata publication: https://doi.org/10.1038/s41597-022-01216-6\nML publication: -\nsource dataset identifier: PXD028735\nspecies: Homo sapiens (Human), Saccharomyces cerevisiae (Baker‚Äôs yeast), Escherichia coli (E. coli)\nsize: 6.268 (small)\nformat: tsv\ncolumns: Modified sequence, Charge, CCS, Ion Mobility, Ion Mobility Units, High Energy, Ion Mobility Offset\nmass modifications: unmodified & oxidation & acetylation & carbamidomethyl\nionmobility_type: TWIMS\n\n\n\nData description\nFrom the original paper:\nAn M-class LC system (Waters Corporation, Milford, MA) was equipped with a 1.7‚Äâ¬µm CSH 130 C18 300‚Äâ¬µm‚Äâ√ó 100‚Äâmm column, operating at 5‚Äâ¬µL/min with a column temperature of 55‚Äâ¬∞C. Mobile phase A was UPLC-grade water containing 0.1% (v/v) FA and 3% DMSO, mobile phase B was ACN containing 0.1% (v/v) FA. Peptides were separated using a linear gradient of 3‚àí30% mobile phase B over 120‚Äâminutes. All experiments were conducted on a Synapt G2-Si mass spectrometer (Waters Corporation, Wilmslow, UK). The ESI Low Flow probe capillary voltage was 3‚ÄâkV, sampling cone 60‚ÄâV, source offset 60‚ÄâV, source temperature 80‚Äâ¬∞C, desolvation temperature 350‚Äâ¬∞C, cone gas 80‚ÄâL/hr, desolvation gas 350‚ÄâL/hr, and nebulizer pressure 2.5‚Äâbar. A lock mass reference signal of GluFibrinopeptide B (m/z 785.8426) was sampled every 30‚Äâs.\n\n\nSample protocol description\nFrom the original paper:\nMass spectrometry-compatible Human K562 (P/N: V6951) and Yeast (P/N: V7461) protein digest extracts were purchased from Promega (Madison, Wisconsin, United States). Lyophilised MassPrep Escherichia.coli digest standard (P/N:186003196) was purchased from Waters Corporation (Milford, Massachusetts, United States). The extracts were reduced with dithiothreitol (DTT), alkylated with iodoacetamide (IAA) and digested with sequencing grade Trypsin(-Lys C) by the respective manufacturers. The digested protein extracts were reconstituted in a mixture of 0.1% Formic acid (FA) in water (Biosolve B.V, Valkenswaard, The Netherlands) and spiked with iRT peptides (Biognosys, Schlieren, Switzerland) at a ratio of 1:20‚Äâv/v. Two master samples A and B were created similar to Navarro et al., each in triplicate, as shown in Fig. 1. Sample A was prepared by mixing Human, Yeast and E.coli at 65%, 30% and 5% weight for weight (w/w), respectively. Sample B was prepared by mixing Human, Yeast and E.coli protein digests at 65%, 15%, 20% w/w, respectively. The resulting samples have logarithmic fold changes (log2FCs) of 0, ‚àí1 and 2 for respectively Human, Yeast and E.coli. One sixth of each of the triplicate master batches of A and B were mixed to create a QC sample, containing 65% w/w Human, 22.5% w/w Yeast and 12.5% w/w E.coli.\n\n\nData analysis protocol\nSee Data description\n\n\nComments:\n/\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/DLOmix.html",
    "href": "datasets/retentiontime/DLOmix.html",
    "title": "DLOmix",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide retention time\ntitle: DLOmix deep learning in proteomics python framework for retention time\ntag: DLOmix_RT\ndata publication: \nML publication: \nsource dataset identifier: \nspecies: Homo sapiens (human)\nsize: 27200 peptides in train, 6000 in test\nformat: CSV\ncolumns: peptide sequence, iRT calibrated retention time (~minutes)\nchromatography_column_type: \nurl: https://pypi.org/project/dlomix/\nurl: https://github.com/wilhelm-lab/dlomix\nurl: https://github.com/wilhelm-lab/dlomix/tree/develop/example_dataset\n\n\n\nData description\nThis dataset is from the DLOmix project, which includes train, validation, and test sets of\n\n\nComments\n\nTutorial here: https://github.com/wilhelm-lab/dlomix/blob/develop/notebooks/Example_RTModel_Walkthrough_colab.ipynb\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/index.html",
    "href": "datasets/retentiontime/index.html",
    "title": "Retention time",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html",
    "href": "datasets/retentiontime/ProteomeTools_RT.html",
    "title": "ProteomeTools",
    "section": "",
    "text": "data type: Peptide retention time\ntitle: ProteomeTools synthetic peptides and iRT calibrated retention times\ntag: ProteomeTools_RT\ndata publication: https://doi.org/10.1038/nmeth.4153\nML publication: https://doi.org/10.1038/s41592-019-0426-7\nsource dataset identifier: PXD004732\nspecies: Homo sapiens (human)\nsize: 100.000 (small), 250.000 (medium), 2.000.000 (large), 250.000 (oxidation), 500.000 (mixed)\nformat: CSV\ncolumns: index, retention time, sequence, modified sequence\nmass modifications: unmodified & oxidation\nchromatography_column_type:\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#data-description",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#data-description",
    "title": "ProteomeTools",
    "section": "data description",
    "text": "data description\nThe ProteomeTools project aims to derive molecular and digital tools from the human proteome to facilitate biomedical and life science research. Here, we describe the generation and multimodal LC-MS/MS analysis of >350,000 synthetic tryptic peptides representing nearly all canonical human gene products. This resource will be extended to 1.4 million peptides within two years and all data will be made available to the public in ProteomicsDB."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#sample-protocol-description",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#sample-protocol-description",
    "title": "ProteomeTools",
    "section": "sample protocol description:",
    "text": "sample protocol description:\nTryptic peptides were individually synthesized by solid phase synthesis, combined into pools of ~1,000 peptides and measured on an Orbitrap Fusion mass spectrometer. For each peptide pool, an inclusion list was generated to target peptides for fragmentation in further LC-MS experiments using five fragmentation methods (HCD, CID, ETD, EThCD, ETciD) with ion trap or Orbitrap readout and HCD spectra were recorded at 6 different collision energies."
  },
  {
    "objectID": "datasets/retentiontime/ProteomeTools_RT.html#data-anaylsis-protocol",
    "href": "datasets/retentiontime/ProteomeTools_RT.html#data-anaylsis-protocol",
    "title": "ProteomeTools",
    "section": "data anaylsis protocol:",
    "text": "data anaylsis protocol:\nLC-MS runs were individually analyzed using MaxQuant 1.5.3.30.\n\nComments\n\nThe full dataset was reduced in size to small, medium, and large sizes\nLink to FigShare area?"
  },
  {
    "objectID": "datasets/retentiontime/Sharma_HeLa_RT.html",
    "href": "datasets/retentiontime/Sharma_HeLa_RT.html",
    "title": "Sharma et al.¬†HeLa",
    "section": "",
    "text": "Attributes\n\ndata type: Peptide retention time\ntitle: Kirill Pevzner ‚ÄúProteomics Retention Time Prediction‚Äù dataset from Sharma et al.¬†HeLa data from kaggle\ntag: Sharma_HeLa_RT\ndata publication: \nML publication: \nsource dataset identifier: \nspecies: Homo sapiens (human)\nsize: 14361 peptides\nformat: TSV\ncolumns: peptide sequence, uncalibrated elution time\nchromatography_column_type: \n\n\n\nData description\nThis dataset was downloaded from kaggle (https://www.kaggle.com/datasets/kirillpe/proteomics-retention-time-prediction) It is a simple list of peptide sequences and uncalibrated retention times in seconds specific to one dataset. The pedigree of the data is not well known.\n\n\nComments\n\nReference is Sharma et al., but exact publication is unknown\nThe kaggle web page lists a filename ‚Äúmod.txt‚Äù, but the data archive includes only one file ‚Äúunmod.txt‚Äù\nThere are 14,361 data lines in the file (plus 1 header line)\nHeader line is ‚Äúsequence RT‚Äù\nNone of the peptides have a mass modification listed\nPresumably this includes only peptides with no mass modification noted in the id, rather than stripped of mods\nAre any peptide sequences repeated?\nThe false discovery rate in peptide identifications is not currently known (presumably some are wrong ids)\nThis may possibly be the unmodified peptides from PXD000612\nThis may possibly be the unmodified peptides from https://pubmed.ncbi.nlm.nih.gov/25159151/\nThis is a phospho-enriched dataset. Perhaps only the unmodified peptides are offered\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "[TODO]\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Publication",
    "section": "",
    "text": "[TODO]\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/index.html",
    "href": "tutorials/fragmentation/index.html",
    "title": "Fragmentation",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "",
    "text": "In bottom-up proteomics, a peptide fragmentation spectrum (MS2) is the most central source of information to identify a peptide (and the original protein by extend). In traditional peptide fragment spectrum identification workflows, only the presence and location (x-axis) of peaks in the spectrum are used to identify the peptide that generated the spectrum. The intensity of these peaks (y-axis) are, however, seldomly used in a comprehensive manner. At most, traditional approaches naively assume that higher intensity is always better. The lack of usage of peptide spectrum intensity patterns in the identification step is mainly due to their complexity. The location of certain peaks (e.g., b- and y-ions) are easily calculated based on their mass for any given peptide. Their intensity, however, follows complex yet predictable patterns that cannot be simply calculated. Nevertheless, they can be predicted with machine learning.\nIn this tutorial you will learn the basic steps in developing a machine learning predictor for peptide fragmentation intensity prediction. The first chapter handles the preparation and parsing of training data and the second chapter first handles training a traditional machine learning model, and then a deep learning model.\nTo avoid an overly complex tutorial, some aspects to intensity prediction are simplified or not handled. For example, the resulting models will only be able to predict singly charged b- and y-ions for unmodified peptides.\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#finding-spectral-libraries",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#finding-spectral-libraries",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.1 Finding spectral libraries",
    "text": "1.1 Finding spectral libraries\nTraining data for peptide fragmentation spectrum intensity prediction consists of spectra that were already identified. The most convenient source of such information are spectral libraries. These are datasets that were compiled from a collection of mass spectrometry runs and usually consist of a single representative spectrum for each peptide that was identified.\nMany precompiled spectral libraries are available online. You can also generate your own from a collection of proteomics experiments, using software such as SpectraST.\nSpectral libraries can be downloaded, for instance, from NIST, the US National Institute of Standards and Technology . For this part of the practical, we will download the 2020 Human HCD library of ‚Äúbest‚Äù tryptic spectra. For ease-of-use, we will download it in the text-based NIST MSP format.\nThe following code cell automatically downloads and extracts the spectral library file using Linux commands. If you are working locally on a Windows machine, you can download and extract the files manually.\nNote the exclamation marks in the code cell below, which allow us to run Linux commands, instead of Python.\n\n# THIS CELL WORKS ONLY ON LINUX HOSTS (e.g. Google Colab)\n\n# `wget` is a Linux command line tool to download files. \n! wget -q https://chemdata.nist.gov/download/peptide_library/libraries/human/HCD/2020_05_19/human_hcd_tryp_best.msp.tar.gz\n\n# Next, we need to unpack the .tar.gz file\n! tar -xf human_hcd_tryp_best.msp.tar.gz\n\n'wget' is not recognized as an internal or external command,\noperable program or batch file.\ntar: Error opening archive: Failed to open 'human_hcd_tryp_best.msp.tar.gz'\n\n\nLet‚Äôs explore the MSP spectral library file, using the Linux head command:\n\n! head human_hcd_tryp_best.msp\n\n'head' is not recognized as an internal or external command,\noperable program or batch file.\n\n\nThis shows the beginning of the first spectrum in the spectral library. Each spectrum entry consists of a header with identification data and metadata, and a peak list with three columns:\n\nm/z values\nintensity values\npeak annotation info\n\nAs the sequence of the first peptide is AAAAAAAAAAAAAAAGAGAGAK, we can assume that this library is ordered alphabetically. You can read through the file to verify this assumption. This ordering will be important later on."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parsing-the-msp-spectral-library-file",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parsing-the-msp-spectral-library-file",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.2 Parsing the MSP spectral library file",
    "text": "1.2 Parsing the MSP spectral library file\nPyteomics is a Python package for proteomics that contains readers for many proteomics-related file formats. Unfortunately, MSP is not one of them. So first, we need a custom MSP reader function.\n\nfrom rich import print, progress  # Rich is a pretty cool library. Google it ;)\nimport numpy as np\nimport pandas as pd\n\nThis function iterates over each line in the MSP file. Once it has gathered all information for a single spectrum, it uses yield to return a dictionary. This means that we can iterate over the function using a for loop, and process spectra one-by-one.\nIf you do not fully understand the function, no problem! This is not the important part of the tutorial üòâ\n\ndef read_msp(filename):\n    \"\"\"Iterate over MSP spectral library file and return spectra as dicts.\"\"\"\n    spectrum = {}\n    mz = []\n    intensity = []\n    annotation = []\n\n    for line in open(filename, \"rt\"):\n        # `Name: ` is the first line of a new entry in the file\n        if line.startswith(\"Name: \"):\n            if spectrum:\n                # Finalize and yield previous spectrum\n                spectrum[\"sequence\"] = spectrum[\"Fullname\"].split(\".\")[1]  # Remove the previous/next amino acids\n                spectrum[\"mz\"] = np.array(mz, dtype=\"float32\")\n                spectrum[\"intensity\"] = np.array(intensity, dtype=\"float32\")\n                spectrum[\"annotation\"] = np.array(annotation, dtype=\"str\")\n                yield spectrum\n\n                # Define new spectrum\n                spectrum = {}\n                mz = []\n                intensity = []\n                annotation = []\n            \n            # Extract everything after `Name: `\n            spectrum[\"Name\"] = line.strip()[6:]\n\n        elif line.startswith(\"Comment: \"):\n            # Parse all comment items as metadata\n            metadata = [i.split(\"=\") for i in line[9:].split(\" \")]\n            for item in metadata:\n                if len(item) == 2:\n                    spectrum[item[0]] = item[1]\n\n        elif line.startswith(\"Num peaks: \"):\n            spectrum[\"Num peaks\"] = int(line.strip()[11:])\n\n        elif len(line.split(\"\\t\")) == 3:\n            # Parse peak list items one-by-one\n            line = line.strip().split(\"\\t\")\n            mz.append(line[0])\n            intensity.append(line[1])\n            annotation.append(line[2].strip('\"'))\n\n    # Final spectrum\n    spectrum[\"sequence\"] = spectrum[\"Fullname\"].split(\".\")[1]  # Remove the previous/next amino acids\n    spectrum[\"mz\"] = np.array(mz, dtype=\"float32\")\n    spectrum[\"intensity\"] = np.array(intensity, dtype=\"float32\")\n    spectrum[\"annotation\"] = np.array(annotation, dtype=\"str\")\n    yield spectrum\n\nLet‚Äôs explore the first spectrum:\n\n# break allows us to only stop after the first spectrum is defined\nfor spectrum in read_msp(\"human_hcd_tryp_best.msp\"):\n    print(spectrum[\"Name\"])\n    break\n\nAAAAAAAAAAAAAAAGAGAGAK/2_0\n\n\n\nWe can format the peak list as a Pandas DataFrame:\n\npd.DataFrame({\n    \"mz\": spectrum[\"mz\"],\n    \"intensity\": spectrum[\"intensity\"],\n    \"annotation\": spectrum[\"annotation\"],\n})\n\n\n\n\n\n  \n    \n      \n      mz\n      intensity\n      annotation\n    \n  \n  \n    \n      0\n      110.071198\n      259243.203125\n      ? 143/200\n    \n    \n      1\n      115.086403\n      97764.398438\n      a2/-1.6ppm 145/200\n    \n    \n      2\n      116.070396\n      26069.500000\n      ? 80/200\n    \n    \n      3\n      120.080597\n      208924.406250\n      ? 148/200\n    \n    \n      4\n      129.065704\n      25535.900391\n      Int/AG/-1.2ppm,Int/GA/-1.2ppm 86/200\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      112\n      1170.621338\n      442693.312500\n      y16/-1.1ppm 180/200\n    \n    \n      113\n      1171.624146\n      173247.703125\n      y16+i/-1.0ppm 133/200\n    \n    \n      114\n      1241.657959\n      264065.593750\n      y17/-1.3ppm 170/200\n    \n    \n      115\n      1242.660156\n      112235.101562\n      y17+i/-1.8ppm 125/200\n    \n    \n      116\n      1312.693848\n      74808.500000\n      y18/-2.3ppm 116/200\n    \n  \n\n117 rows √ó 3 columns\n\n\n\nThe left-most column denotes the peak annotation. This tells us which ion generated the peak, according to the search engine or library generation software. Note that many peaks ‚Äî highlighted with a question mark ‚Äî are not annotated, even though the spectrum was confidently identified.\nUsing the Python package spectrum_utils, we can easily visualize the spectrum:\n\nimport matplotlib.pyplot as plt\n\nimport spectrum_utils.spectrum as sus\nimport spectrum_utils.plot as sup\n\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"]\n    )\n)\nplt.show()"
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#preparing-spectra-for-training",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#preparing-spectra-for-training",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.3 Preparing spectra for training",
    "text": "1.3 Preparing spectra for training\nTo use a peptide fragmentation spectrum such as this one as training target for a machine learning model, it needs some preparation and parsing. Usually this comprises of the following steps:\n\nNormalize the intensities\nTransform the intensities\nAnnotate the peaks\nParse the relevant peak intensities to an format suitable for machine learning\n\nFor each of these steps, we will write a function that can be reused later on in the tutorial.\n\n1.3.1 Normalize the intensities\nDepending on the file format, peak intensities can range from 0 to 1, from 0 to 100, from 0 from 10 000‚Ä¶ Machine learning algorithms require the target (and feature) values to be normalized in a specific range. For fragmentation spectra, there are two common options: total ion current (TIC) normalization and base peak normalization. For the former, all intensity values are divided by the total sum of all intensity values in the spectrum. The sum of all normalized intensities will be 1. For the latter, all intensity values are divided by the most intense peak in the spectrum, resulting in that peak to have normalized intensity 1. Here we will implement TIC-normalization.\n\ndef tic_normalize(msp_spectrum):\n    tic = np.sum(msp_spectrum[\"intensity\"])\n    msp_spectrum[\"intensity\"] = msp_spectrum[\"intensity\"] / tic\n    return msp_spectrum\n\n\n# Before normalization\nspectrum[\"intensity\"][:10]\n\narray([259243.2,  97764.4,  26069.5, 208924.4,  25535.9, 361336.8,\n       120990.5, 401263.5,  54146.8, 259764.2], dtype=float32)\n\n\n\nspectrum = tic_normalize(spectrum)\n\n# After normalization\nspectrum[\"intensity\"][:10]\n\narray([0.00882945, 0.00332971, 0.00088789, 0.00711566, 0.00086972,\n       0.0123066 , 0.00412076, 0.01366645, 0.00184416, 0.00884719],\n      dtype=float32)\n\n\n\n\n1.3.2 Transform the intensities\nThe distribution of peak intensities shows us that most peptide fragmentation peaks have a relatively low intensity, while only a few peaks are more intense:\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n# Before transform\nsns.displot(spectrum[\"intensity\"], bins=20)\nplt.show()\n\n\n\n\nTo make the intensities follow a more linear distribution ‚Äî which is better for machine learning algorithms ‚Äî we can transform the intensity values. Two methods are often used: square root-tranform, and log-transform. While both methods mostly have the same effect, we will here opt for square root transform, as log-transform results in negative values, which can be cumbersome to deal with.\n\ndef sqrt_transform(msp_spectrum):\n    msp_spectrum[\"intensity\"] = np.sqrt(msp_spectrum[\"intensity\"])\n    return msp_spectrum\n\n\nspectrum = sqrt_transform(spectrum)\n\n# After transform\nsns.displot(spectrum[\"intensity\"], bins=20)\nplt.show()\n\n\n\n\n\n\n1.3.3 Annotate the peaks\nWith the NIST spectral libraries, this step is pretty easy, as peak annotations are already present. If this would not be the case, we can make use of spectrum_utils, which can annotate peaks given the peptide sequence and any modifications. See the spectrum_utils documentation for more info.\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"],\n        peptide=spectrum[\"sequence\"],\n    ).annotate_peptide_fragments(25, \"ppm\")\n)\nplt.show()\n\n\n\n\n\n\n1.3.4 Parse the relevant peak intensities to an format suitable for machine learning\nNote in the visualization above that spectrum_utils only annotated b- and y-ions, while in the MSP file many other ion types are also annotated. For simplicity‚Äôs sake, in this tutorial we will train a model to only predict singly charged b- and y-ions.\nLet‚Äôs filter the spectrum for only those peaks. This can be done with regular expressions (regex) and numpy. TIP: regex101.com is a great website for building and testing regular expressions. The regex ^(b|y)([0-9]+)\\/ only matches peak annotations for singly charged b- and y-ions. You can investigate it at https://regex101.com/r/bgZ7EG/1.\nIn the following function, numpy.vectorize is used. What does it do and why do we use it here?\n\nimport re\n\ndef filter_peaks(msp_spectrum):\n    \"\"\"Filter spectrum peaks to only charge 1 b- and y ions.\"\"\"\n    # Generate the boolean mask\n    get_mask = np.vectorize(lambda x: bool(re.match(\"^(b|y)([0-9]+)\\/\", x)))\n    mask = get_mask(msp_spectrum[\"annotation\"])\n    \n    # Apply the mask to each peak array\n    msp_spectrum[\"annotation\"] = msp_spectrum[\"annotation\"][mask]\n    msp_spectrum[\"mz\"] = msp_spectrum[\"mz\"][mask]\n    msp_spectrum[\"intensity\"] = msp_spectrum[\"intensity\"][mask]\n\n    return msp_spectrum\n\nspectrum = filter_peaks(spectrum)\n\n\nplt.figure(figsize=(12,6))\nsup.spectrum(\n    sus.MsmsSpectrum(\n        identifier=spectrum[\"Name\"],\n        precursor_mz=float(spectrum[\"Parent\"]),\n        precursor_charge=int(spectrum[\"Charge\"]),\n        mz=spectrum[\"mz\"],\n        intensity=spectrum[\"intensity\"],\n        peptide=spectrum[\"sequence\"]\n    ).annotate_peptide_fragments(25, \"ppm\")\n)\nplt.show()\n\n\n\n\nNow, the spectrum indeed only contains singly charged b- and y-ions. Note the nice gausian-like distributions of equally-distanced b- and y-ions. This is a feature specific for this peptide spectrum. Can you guess why? Tip: Take a look at the peptide sequence.\nCurrently, all peaks are listed together in single numpy arrays, sorted by m/z values. For training a machine learning model, we need the intensity values in a more suitable structure. As we are planning to only predict simple singly charged b- and y-ions, we can create two arrays ‚Äî one for each ion type ‚Äî with the ions sorted by ion number:\nparsed_intensity = {\n    \"b\": [b1, b2, b3, b4 ... bN],\n    \"y\": [y1, y2, y3, y4 ... yN]\n}\nwhere N is the total number of possible fragments for that peptide sequence. Quick question: What value will N have for our peptide with sequence AAAAAAAAAAAAAAAGAGAGAK?\nThe following function builds upon the filter_peaks function to not only filter the correct ion types, but also order them properly:\n\ndef parse_peaks(msp_spectrum, ion_type):\n    # Generate vectorized functions\n    get_ions = np.vectorize(lambda x: bool(re.match(f\"^({ion_type})([0-9]+)\\/\", x)))\n    get_ion_order = np.vectorize(lambda x: re.match(f\"^({ion_type})([0-9]+)\\/\", x)[2])\n\n    # Get mask with requested ion types\n    mask = get_ions(msp_spectrum[\"annotation\"])\n\n    # Create empty array with for all possible ions\n    n_ions = len(msp_spectrum[\"sequence\"]) - 1\n    parsed_intensity = np.zeros(n_ions)\n\n    # Check if any ions of this type are present\n    if mask.any():\n        # Filter for ion type and sort\n        ion_order = get_ion_order(msp_spectrum[\"annotation\"][mask]).astype(int) - 1\n        # Add ions to correct positions in new array\n        parsed_intensity[ion_order] = msp_spectrum[\"intensity\"][mask]\n\n    try:\n        msp_spectrum[\"parsed_intensity\"][ion_type] = parsed_intensity\n    except KeyError:\n        msp_spectrum[\"parsed_intensity\"] = {}\n        msp_spectrum[\"parsed_intensity\"][ion_type] = parsed_intensity\n    \n    return msp_spectrum\n\nspectrum = parse_peaks(spectrum, \"b\")\nspectrum = parse_peaks(spectrum, \"y\")\n\n\nspectrum['parsed_intensity']\n\n{'b': array([0.        , 0.0940595 , 0.18064232, 0.20420307, 0.23347196,\n        0.2457854 , 0.23112106, 0.20064339, 0.16306745, 0.1246587 ,\n        0.08999325, 0.05416884, 0.        , 0.        , 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        ]),\n 'y': array([0.09027135, 0.03876459, 0.09092397, 0.07086667, 0.1299265 ,\n        0.09038813, 0.15890096, 0.13701038, 0.13768263, 0.14171469,\n        0.15388304, 0.16281605, 0.16425258, 0.15970773, 0.1443574 ,\n        0.12279043, 0.09483507, 0.05047642, 0.        , 0.        ,\n        0.        ])}\n\n\nGreat! These values are now ready to be used as prediction targets for a machine learning algorithm."
  },
  {
    "objectID": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parse-the-full-spectral-library",
    "href": "tutorials/fragmentation/nist-1-parsing-spectral-library.html#parse-the-full-spectral-library",
    "title": "NIST (part 1): Parsing the spectral library",
    "section": "1.4 Parse the full spectral library",
    "text": "1.4 Parse the full spectral library\nNow that all functions for spectrum preparation are written, we can parse the full spectral library. Let‚Äôs first explore some of the basic statistics of this library.\n\n1.4.1 Exploring basic spectral library statistics\n\nTotal number of spectra\nThis one is easy, we quickly scan the number of lines starting with Name: in the file:\n\ndef msp_count_spectra(filename):\n    \"\"\"Count number of spectra in MSP file.\"\"\"\n    count = 0\n    for line in open(filename, \"rt\"):\n        if line.startswith(\"Name: \"):\n            count += 1\n    return count\n\nn_spectra = msp_count_spectra(\"human_hcd_tryp_best.msp\")\nprint(n_spectra)\n\n398373\n\n\n\nFor more statistics, we will have to read the full library. One advantage we have now, is that knowing the total number of spectra allows us to track the progress of reading the full MSP file (progress.track). To limit the amount of data we keep in memory (this full MSP file is almost 2GB!), we can process the intensity values of each spectrum while parsing and only keep the parsed data:\n\nspectrum_list = []\nfor msp_spectrum in progress.track(\n    read_msp(\"human_hcd_tryp_best.msp\"),\n    total=n_spectra,\n    description=\"Parsing  MSP file...\"\n):\n    # Process intensities\n    msp_spectrum = tic_normalize(msp_spectrum)\n    msp_spectrum = sqrt_transform(msp_spectrum)\n    msp_spectrum = parse_peaks(msp_spectrum, \"b\")  # Adds `parsed_intensity` > `b`\n    msp_spectrum = parse_peaks(msp_spectrum, \"y\")  # Adds `parsed_intensity` > `y`\n\n    # Parse metadata\n    spectrum = {\n        \"sequence\": msp_spectrum[\"sequence\"],\n        \"modifications\": msp_spectrum[\"Mods\"],\n        \"charge\": int(msp_spectrum[\"Charge\"]),\n        \"nce\": float(msp_spectrum[\"NCE\"]),\n        \"parsed_intensity\": msp_spectrum[\"parsed_intensity\"]\n    }\n\n    # Append to list\n    spectrum_list.append(spectrum)\n\nc:\\Users\\ralfg\\miniconda3\\envs\\proteomicsml\\lib\\site-packages\\rich\\live.py:229: UserWarning: install \"ipywidgets\" \nfor Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\n\n\n\nGenerating a Pandas DataFrame from the list of spectrum dictionaries, allows us to easily explore the full dataset:\n\nspectrum_df = pd.DataFrame(spectrum_list)\nspectrum_df\n\n\n\n\n\n  \n    \n      \n      sequence\n      modifications\n      charge\n      nce\n      parsed_intensity\n    \n  \n  \n    \n      0\n      AAAAAAAAAAAAAAAGAGAGAK\n      0\n      2\n      29.43\n      {'b': [0.0, 0.09405950456857681, 0.18064232170...\n    \n    \n      1\n      AAAAAAAAAAAAAAAGAGAGAK\n      0\n      3\n      29.22\n      {'b': [0.0, 0.21546243131160736, 0.21998108923...\n    \n    \n      2\n      AAAAAAAAAAAPPAPPEGASPGDSAR\n      0\n      2\n      27.80\n      {'b': [0.0, 0.0, 0.056045547127723694, 0.10302...\n    \n    \n      3\n      AAAAAAAAAAAPPAPPEGASPGDSAR\n      0\n      3\n      30.00\n      {'b': [0.0, 0.04407356679439545, 0.07545641809...\n    \n    \n      4\n      AAAAAAAAAASGAAIPPLIPPR\n      0\n      3\n      0.00\n      {'b': [0.0, 0.10330961644649506, 0.15637055039...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      398368\n      YYYYHR\n      0\n      2\n      30.20\n      {'b': [0.0, 0.14489535987377167, 0.0, 0.0, 0.0...\n    \n    \n      398369\n      YYYYHR\n      0\n      3\n      37.52\n      {'b': [0.018267542123794556, 0.076188296079635...\n    \n    \n      398370\n      YYYYMWK\n      0\n      2\n      31.00\n      {'b': [0.0, 0.22406582534313202, 0.11588517576...\n    \n    \n      398371\n      YYYYMWK\n      1(4,M,Oxidation)\n      2\n      30.00\n      {'b': [0.0, 0.14110229909420013, 0.0, 0.0, 0.0...\n    \n    \n      398372\n      YYYYWHLR\n      0\n      3\n      36.22\n      {'b': [0.0, 0.0886630266904831, 0.0, 0.0, 0.0,...\n    \n  \n\n398373 rows √ó 5 columns\n\n\n\nMaking a Pandas DataFrame out of spectrum_list is so simple because it is a list of consistent dictionaries.\n\n\nPrecursor charge state\nA different precursor charge state can heavily alter peptide fragmentation. It is therefore important to have a representative amount of peptide spectra for each charge state in the spectral library.\n\nsns.countplot(data=spectrum_df, x=\"charge\")\nplt.show()\n\n\n\n\n\n\nPeptide length\nIdem for the length of the peptide sequence. It usually makes sense to filter the train dataset for peptides within a certain length range.\n\nsns.kdeplot(spectrum_df[\"sequence\"].str.len())\nplt.xlabel(\"Sequence length\")\nplt.show()\n\n\n\n\n\nspectrum_df[\"sequence\"].str.len().describe()\n\ncount    398373.000000\nmean         15.541467\nstd           6.506968\nmin           6.000000\n25%          11.000000\n50%          14.000000\n75%          19.000000\nmax          50.000000\nName: sequence, dtype: float64\n\n\n\n(spectrum_df[\"sequence\"].str.len() > 35).value_counts()\n\nFalse    393429\nTrue       4944\nName: sequence, dtype: int64\n\n\nFor this dataset, the minimum peptide length is 6, while the maximum is 50. Nevertheless, only\n\nPeptide modifications\nLikewise, peptide modifications can influence peptide fragmentation. How many of the spectra in our library come from modified peptides?\n\nmodification_state = (spectrum_df[\"modifications\"] == \"0\").map({True: \"Unmodified\", False: \"Modified\"})\nsns.countplot(x=modification_state)\nplt.show()\n\n\n\n\n\n\n\nCollision energy\n\nsns.histplot(spectrum_df[\"nce\"], bins=30)\nplt.xlabel(\"NCE\")\nplt.show()\n\n\n\n\nNote the range of the x-axis, which was automatically chosen by the plotting library. It seems to start at 0, which indicates that some values are very low‚Ä¶\n\n(spectrum_df[\"nce\"] == 0.0).value_counts()\n\nFalse    398103\nTrue        270\nName: nce, dtype: int64\n\n\nIndeed, it seems that some peptide spectra have collision energy (CE) 0, which most likely means that the true collision energy setting is not known. We can either opt to not use CE as a feature for training, or to remove these spectra from the dataset. Including these values would introduce unwanted noise in the training data.\n\n\nDuplicate entries?\nAn important aspect to compiling training data for machine learning is whether or not entries are duplicated. With spectral libraries, matters are complicated by multiple levels of ‚Äúuniqueness‚Äù:\n\nPeptide level: Unique sequence\nPeptidoform level: Unique sequence & modifications\nPrecursor level: Unique sequence & modifications & charge\n\nMore parameters can be included for ‚Äúuniqueness‚Äù, such as instrument and acquisition properties: CE, fragmentation method (beam-type CID (‚ÄúHCD‚Äù), trap-type CID, ETD, EAD‚Ä¶), acquisition method (Orbitrap, ion trap, TOF‚Ä¶). In this tutorial, we are using only HCD Orbitrap data, which makes things a bit simpler. Nevertheless, this will impact the application domain of the final models.\n\ncounts = pd.DataFrame({\n    \"Level\": [\n        \"Full library\",\n        \"Precursor\",\n        \"Peptidoform\",\n        \"Peptide\",\n    ],\n    \"Count\": [\n        spectrum_df.shape[0],\n        spectrum_df[[\"sequence\", \"modifications\", \"charge\"]].drop_duplicates().shape[0],\n        spectrum_df[[\"sequence\", \"modifications\"]].drop_duplicates().shape[0],\n        spectrum_df[\"sequence\"].unique().shape[0],\n    ],\n})\n\n\nsns.barplot(data=counts, x=\"Level\", y=\"Count\")\nplt.show()\n\n\n\n\n\ncounts\n\n\n\n\n\n  \n    \n      \n      Level\n      Count\n    \n  \n  \n    \n      0\n      Full library\n      398373\n    \n    \n      1\n      Precursor\n      398373\n    \n    \n      2\n      Peptidoform\n      292061\n    \n    \n      3\n      Peptide\n      257202\n    \n  \n\n\n\n\nSeems like this library was filtered for uniqueness on the precursor level.\n\n\n\n1.4.1 Selecting data\nFor selecting training data, we will apply some additional filters:\n\nWhile plain amino acid sequences are straightforward to encode, peptide modifications complicate matters. For simplicity‚Äôs sake, we will therefore not open the ‚Äúcan of modifications‚Äù in this tutorial.\nAs we might want to use CE as a feature, we can remove the small amount of entries that are missing the a CE value\nTo make the training task a bit less complex, we can limit peptide length to 35. Although the maximum peptide length in this library is 50, only 4944 spectra have a peptide length of over 35.\n\n\nspectrum_df = spectrum_df[\n    (modification_state == \"Unmodified\") &\n    (spectrum_df[\"sequence\"].str.len() <= 35) &\n    (spectrum_df[\"nce\"] != 0)\n]\n\nLet‚Äôs see how many spectra we retained:\n\nspectrum_df.shape[0]\n\n270440\n\n\n\nspectrum\n\n{'sequence': 'YYYYWHLR',\n 'modifications': '0',\n 'charge': 3,\n 'nce': 36.22,\n 'parsed_intensity': {'b': array([0.        , 0.08866303, 0.        , 0.        , 0.        ,\n         0.        , 0.        ]),\n  'y': array([0.16996175, 0.12056533, 0.16980277, 0.20397678, 0.18093778,\n         0.09508998, 0.        ])}}\n\n\n\n\n1.4.2 Train / validation / test split\nNow that we have our data, we can filter it to a final set for training and validation and a final set for testing. A small reminder of what these terms mean:\n\nTraining data: For training the model\nValidation data: For validating the model while optimizing hyperparameters\nTesting data: For final testing of model that was trained with the best hyperparameters (according to the validation data), right before deployment\n\nThe testing data cannot be used until a final model is trained, and serves as a last test before deployment. It should not be used before a final model is selected.\n\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed()  # Why is this needed?\n\ntrain_val_peptides, test_peptides = train_test_split(spectrum_df[\"sequence\"].unique(), train_size=0.9)\ntrain_val_spectra = spectrum_df[spectrum_df[\"sequence\"].isin(train_val_peptides)]\ntest_spectra = spectrum_df[spectrum_df[\"sequence\"].isin(test_peptides)]\n\nQuestion: Why do we not apply train_test_split() directly on spectrum_df, but instead on spectrum_df[\"sequence\"].unique()?"
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\namino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\naa_properties = {\n    \"basicity\": np.array([37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104]),\n    \"helicity\": np.array([68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55]),\n    \"hydrophobicity\": np.array([51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70]),\n    \"pI\": np.array([32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28]),\n}\n\nproperties_df = pd.DataFrame(aa_properties, index=amino_acids)\nproperties_df\n\n\n\n\n\n  \n    \n      \n      basicity\n      helicity\n      hydrophobicity\n      pI\n    \n  \n  \n    \n      A\n      37\n      68\n      51\n      32\n    \n    \n      C\n      35\n      23\n      75\n      23\n    \n    \n      D\n      59\n      33\n      25\n      0\n    \n    \n      E\n      129\n      29\n      35\n      4\n    \n    \n      F\n      94\n      70\n      100\n      27\n    \n    \n      G\n      0\n      58\n      16\n      32\n    \n    \n      H\n      210\n      41\n      3\n      48\n    \n    \n      I\n      81\n      73\n      94\n      32\n    \n    \n      K\n      191\n      32\n      0\n      69\n    \n    \n      L\n      81\n      73\n      94\n      32\n    \n    \n      M\n      106\n      66\n      82\n      29\n    \n    \n      N\n      101\n      38\n      12\n      26\n    \n    \n      P\n      117\n      0\n      0\n      35\n    \n    \n      Q\n      115\n      40\n      22\n      28\n    \n    \n      R\n      343\n      39\n      22\n      79\n    \n    \n      S\n      49\n      44\n      21\n      29\n    \n    \n      T\n      90\n      53\n      39\n      28\n    \n    \n      V\n      60\n      71\n      80\n      31\n    \n    \n      W\n      134\n      51\n      98\n      31\n    \n    \n      Y\n      104\n      55\n      70\n      28\n    \n  \n\n\n\n\n\n# Peptide input\n# Feature engineering settings\n\nproperties = np.array([\n    [37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104],  # basicity\n    [68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55],  # helicity\n    [51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70],  # hydrophobicity\n    [32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28],  # pI\n])\n\nquantiles = [0, 0.25, 0.5, 0.75, 1]\naa_indices = {aa: i for i, aa in  enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\naa_to_index = np.vectorize(lambda aa: aa_indices[aa])\n\ndef encode_peptide(sequence, charge):\n    # 4 properties * 5 quantiles * 3 ion types + 4 properties * 4 site + 2 global\n    n_features = 78\n    n_ions = len(sequence) - 1\n\n    # Encode amino acids as integers to index amino acid properties for peptide sequence\n    peptide_indexed = aa_to_index(np.array(list(sequence)))\n    peptide_properties = properties[:, peptide_indexed]\n\n    # Empty peptide_features array\n    peptide_features = np.full((n_ions, n_features), np.nan)\n\n    for b_ion_number in range(1, n_ions + 1):\n        # Calculate quantiles of features across peptide, b-ion, and y-ion\n        peptide_quantiles = np.hstack(\n            np.quantile(peptide_properties, quantiles, axis=1).transpose()\n        )\n        b_ion_quantiles = np.hstack(\n            np.quantile(peptide_properties[:,:b_ion_number], quantiles, axis=1).transpose()\n        )\n        y_ion_quantiles = np.hstack(\n            np.quantile(peptide_properties[:,b_ion_number:], quantiles, axis=1).transpose()\n        )\n\n        # Properties on specific sites: nterm, frag-1, frag+1, cterm\n        specific_site_indexes = np.array([0, b_ion_number - 1, b_ion_number, -1])\n        specific_site_properties = np.hstack(peptide_properties[:, specific_site_indexes].transpose())\n\n        # Global features: Length and charge\n        global_features = np.array([len(sequence), int(charge)])\n\n        # Assign to peptide_features array\n        peptide_features[b_ion_number - 1, 0:20] = peptide_quantiles\n        peptide_features[b_ion_number - 1, 20:40] = b_ion_quantiles\n        peptide_features[b_ion_number - 1, 40:60] = y_ion_quantiles\n        peptide_features[b_ion_number - 1, 60:76] = specific_site_properties\n        peptide_features[b_ion_number - 1, 76:78] = global_features\n\n    return peptide_features\n\n\ndef generate_feature_names():\n    feature_names = []\n    for level in [\"peptide\", \"b\", \"y\"]:\n        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n            for quantile in [\"min\", \"q1\", \"q2\", \"q3\", \"max\"]:\n                feature_names.append(\"_\".join([level, aa_property, quantile]))\n    for site in [\"nterm\", \"fragmin1\", \"fragplus1\", \"cterm\"]:\n        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n            feature_names.append(\"_\".join([site, aa_property]))\n        \n    feature_names.extend([\"length\", \"charge\"])\n    return feature_names\n\nLet‚Äôs test it with a single peptide:\n\npeptide_features = pd.DataFrame(encode_peptide(\"RALFGARIELS\", 2), columns=generate_feature_names())\npeptide_features\n\n\n\n\n\n  \n    \n      \n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      peptide_helicity_max\n      ...\n      fragplus1_basicity\n      fragplus1_helicity\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n    \n  \n  \n    \n      0\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      37.0\n      68.0\n      51.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      1\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      2\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      94.0\n      70.0\n      100.0\n      27.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      3\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      0.0\n      58.0\n      16.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      4\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      37.0\n      68.0\n      51.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      5\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      343.0\n      39.0\n      22.0\n      79.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      6\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      7\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      129.0\n      29.0\n      35.0\n      4.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      8\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      81.0\n      73.0\n      94.0\n      32.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n    \n      9\n      0.0\n      43.0\n      81.0\n      111.5\n      343.0\n      29.0\n      41.5\n      68.0\n      71.5\n      73.0\n      ...\n      49.0\n      44.0\n      21.0\n      29.0\n      49.0\n      44.0\n      21.0\n      29.0\n      11.0\n      2.0\n    \n  \n\n10 rows √ó 78 columns\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#getting-the-target-intensities",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#getting-the-target-intensities",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "2.2 Getting the target intensities",
    "text": "2.2 Getting the target intensities\n\npeptide_targets =  pd.DataFrame({\n    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"],\n})\npeptide_targets\n\n\n\n\n\n  \n    \n      \n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      0.000000\n      0.169962\n    \n    \n      1\n      0.088663\n      0.120565\n    \n    \n      2\n      0.000000\n      0.169803\n    \n    \n      3\n      0.000000\n      0.203977\n    \n    \n      4\n      0.000000\n      0.180938\n    \n    \n      5\n      0.000000\n      0.095090\n    \n    \n      6\n      0.000000\n      0.000000\n    \n  \n\n\n\n\n\npeptide_targets =  pd.DataFrame({\n    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"][::-1],\n})\npeptide_targets\n\n\n\n\n\n  \n    \n      \n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      0.000000\n      0.000000\n    \n    \n      1\n      0.088663\n      0.095090\n    \n    \n      2\n      0.000000\n      0.180938\n    \n    \n      3\n      0.000000\n      0.203977\n    \n    \n      4\n      0.000000\n      0.169803\n    \n    \n      5\n      0.000000\n      0.120565\n    \n    \n      6\n      0.000000\n      0.169962\n    \n  \n\n\n\n\n\nfeatures = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\ntargets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\nspectrum_id = np.full(shape=(targets.shape[0], 1), fill_value=1, dtype=np.uint32)  # Repeat id for all ions\n\n\npd.DataFrame(np.hstack([spectrum_id, features, targets]), columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\n\n\n\n\n\n  \n    \n      \n      spectrum_id\n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      ...\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.088663\n      0.095090\n    \n    \n      2\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.180938\n    \n    \n      3\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      98.0\n      31.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.203977\n    \n    \n      4\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      3.0\n      48.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169803\n    \n    \n      5\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      94.0\n      32.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.120565\n    \n    \n      6\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      22.0\n      79.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169962\n    \n  \n\n7 rows √ó 81 columns\n\n\n\nNote the [::-1] after spectrum[\"parsed_intensity\"][\"y\"]. Remember why we do this?\nLet‚Äôs get a full feature/target table for all spectra in our dataset. Note that this might take some time, sometimes up to 30 minutes. To skip this step, simple download the file with pre-encoded features and targets, and load in two cells below.\n\ntables = []\nfor i, spectrum in progress.track(enumerate(spectrum_list)):\n    features = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\n    targets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n    spectrum_id = np.full(shape=(targets.shape[0], i), fill_value=1, dtype=np.uint32)  # Repeat id for all ions\n    table = np.hstack([spectrum_id, features, targets])\n    tables.append(table)\n\nfull_table = np.vstack(tables)\n\nspectra_encoded = pd.DataFrame(full_table, columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\nspectra_encoded.to_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")\n\nc:\\Users\\ralfg\\miniconda3\\envs\\proteomicsml\\lib\\site-packages\\rich\\live.py:229: UserWarning: install \"ipywidgets\" \nfor Jupyter support\n  warnings.warn('install \"ipywidgets\" for Jupyter support')\n\n\n\n\n\n\n\n\n\n\n# Uncomment this step to load in pre-encoded features from a file:\n# spectra_encoded = pd.read_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")\n\n\nspectra_encoded\n\n\n\n\n\n  \n    \n      \n      spectrum_id\n      peptide_basicity_min\n      peptide_basicity_q1\n      peptide_basicity_q2\n      peptide_basicity_q3\n      peptide_basicity_max\n      peptide_helicity_min\n      peptide_helicity_q1\n      peptide_helicity_q2\n      peptide_helicity_q3\n      ...\n      fragplus1_hydrophobicity\n      fragplus1_pi\n      cterm_basicity\n      cterm_helicity\n      cterm_hydrophobicity\n      cterm_pi\n      length\n      charge\n      b_target\n      y_target\n    \n  \n  \n    \n      0\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.000000\n      0.000000\n    \n    \n      1\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.094060\n      0.000000\n    \n    \n      2\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.180642\n      0.000000\n    \n    \n      3\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.204203\n      0.050476\n    \n    \n      4\n      1.0\n      0.0\n      37.0\n      37.0\n      37.0\n      191.0\n      32.0\n      68.0\n      68.0\n      68.0\n      ...\n      51.0\n      32.0\n      191.0\n      32.0\n      0.0\n      69.0\n      22.0\n      2.0\n      0.233472\n      0.094835\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5792923\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      70.0\n      28.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.180938\n    \n    \n      5792924\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      98.0\n      31.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.203977\n    \n    \n      5792925\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      3.0\n      48.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169803\n    \n    \n      5792926\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      94.0\n      32.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.120565\n    \n    \n      5792927\n      1.0\n      81.0\n      104.0\n      104.0\n      153.0\n      343.0\n      39.0\n      48.5\n      55.0\n      55.0\n      ...\n      22.0\n      79.0\n      343.0\n      39.0\n      22.0\n      79.0\n      8.0\n      3.0\n      0.000000\n      0.169962\n    \n  \n\n5792928 rows √ó 81 columns\n\n\n\nThis is the data we will use for training. Note that each spectrum comprises of multiple lines: One line per b/y-ion couple. The only thing left to do is to split the data into train, validation, and test sets, according to the peptide-level split we made earlier.\n\nspectra_encoded_trainval = spectra_encoded[spectra_encoded.index.isin(train_val_spectra.index)]\nspectra_encoded_test = spectra_encoded[spectra_encoded.index.isin(test_spectra.index)]"
  },
  {
    "objectID": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#hyperparameter-optimization-and-model-selection",
    "href": "tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html#hyperparameter-optimization-and-model-selection",
    "title": "NIST (part 2): Traditional ML: Gradient boosting",
    "section": "2.3 Hyperparameter optimization and model selection",
    "text": "2.3 Hyperparameter optimization and model selection\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\nreg =  GradientBoostingRegressor()\n\nX_train = spectra_encoded_trainval.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\ny_train = spectra_encoded_trainval[\"b_target\"]\nX_test = spectra_encoded_test.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\ny_test = spectra_encoded_test[\"b_target\"]\n\nreg.fit(X_test, y_test)\n\nGradientBoostingRegressor()\n\n\n\ny_test_pred = reg.predict(X_test)\n\n\nnp.corrcoef(y_test, y_test_pred)[0][1]\n\n0.7504125450838988\n\n\nLet‚Äôs see if we can do better by optimizing some hyperparameters!\n\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\n\n\ndef objective(n_estimators):\n    # Define algorithm\n    reg =  GradientBoostingRegressor(n_estimators=n_estimators)\n\n    # Fit model\n    reg.fit(X_test, y_test)\n\n    # Test model\n    y_test_pred = reg.predict(X_test)\n    correlation = np.corrcoef(y_test, y_test_pred)[0][1]\n    \n    return {'loss': -correlation, 'status': STATUS_OK}\n    \n\n\nbest_params = fmin(\n  fn=objective,\n  space=hp.randint('n_estimators', 10, 1000),\n  algo=tpe.suggest,\n  max_evals=10,\n)\n\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [08:50<00:00, 53.01s/trial, best loss: -0.8465849146319573]\n\n\n\nbest_params\n\n{'n_estimators': 874}\n\n\nSuccess! Initially, the default value of 100 estimators was used. According to this hyperopt run, using 874 estimators results in a more performant model."
  },
  {
    "objectID": "tutorials/fragmentation/nist-3-deep-learning-lstm.html",
    "href": "tutorials/fragmentation/nist-3-deep-learning-lstm.html",
    "title": "NIST (part 3): Deep learning: BiLSTM",
    "section": "",
    "text": "[TODO]\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/fragmentation/proteometools-prosit.html",
    "href": "tutorials/fragmentation/proteometools-prosit.html",
    "title": "ProteomicsML.org",
    "section": "",
    "text": "# Load ProteomeTools data from Figshare\n!wget https://figshare.com/ndownloader/files/12506534 \n!mv 12506534 prosit_2018_holdout.hdf5\n\n--2022-09-01 12:36:41--  https://figshare.com/ndownloader/files/12506534\nResolving figshare.com (figshare.com)... 52.30.192.85, 52.30.212.171, 2a05:d018:1f4:d000:d7f8:e0ce:c23a:78f4, ...\nConnecting to figshare.com (figshare.com)|52.30.192.85|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/12506534/holdout_hcd.hdf5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220901/eu-west-1/s3/aws4_request&X-Amz-Date=20220901T123642Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=5cca5e85e0d84c4fccac3f88796a15debbe65ee4585b0b39534eccc6c11117c8 [following]\n--2022-09-01 12:36:42--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/12506534/holdout_hcd.hdf5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220901/eu-west-1/s3/aws4_request&X-Amz-Date=20220901T123642Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=5cca5e85e0d84c4fccac3f88796a15debbe65ee4585b0b39534eccc6c11117c8\nResolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.96.162\nConnecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.96.162|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 262781348 (251M) [application/octet-stream]\nSaving to: ‚Äò12506534‚Äô\n\n12506534            100%[===================>] 250.61M  13.8MB/s    in 20s     \n\n2022-09-01 12:37:03 (12.7 MB/s) - ‚Äò12506534‚Äô saved [262781348/262781348]\n\n\n\n\n# Import packages\nimport pandas as pd\nimport numpy as np\nimport h5py as h5\n\n# Using the alphabet as defined in Prosit: \n# https://github.com/kusterlab/prosit/blob/master/prosit/constants.py#L21-L43\nPROSIT_ALHABET = {\n    \"A\": 1,\n    \"C\": 2,\n    \"D\": 3,\n    \"E\": 4,\n    \"F\": 5,\n    \"G\": 6,\n    \"H\": 7,\n    \"I\": 8,\n    \"K\": 9,\n    \"L\": 10,\n    \"M\": 11,\n    \"N\": 12,\n    \"P\": 13,\n    \"Q\": 14,\n    \"R\": 15,\n    \"S\": 16,\n    \"T\": 17,\n    \"V\": 18,\n    \"W\": 19,\n    \"Y\": 20,\n    \"M(ox)\": 21,\n}\nPROSIT_INDEXED_ALPHABET = {i: c for c, i in PROSIT_ALHABET.items()}\n\n\n# Read the downloaded data to a dataframe\nwith h5.File('prosit_2018_holdout.hdf5', 'r') as f:\n  KEY_ARRAY = [\"sequence_integer\", \"precursor_charge_onehot\", \"intensities_raw\"]\n  KEY_SCALAR = [\"collision_energy_aligned_normed\", \"collision_energy\"]\n  df = pd.DataFrame({key: list(f[key][...]) for key in KEY_ARRAY})\n  for key in KEY_SCALAR:\n    df[key] = f[key][...]\n\n# Add convenience columns\ndf['precursor_charge'] = df.precursor_charge_onehot.map(lambda a: a.argmax() + 1)\ndf['sequence_maxquant'] = df.sequence_integer.map(lambda s: \"\".join(PROSIT_INDEXED_ALPHABET[i] for i in s if i != 0))\ndf['sequence_length'] = df.sequence_integer.map(lambda s: np.count_nonzero(s))\n\n\n\n\n\ndf['precursor_charge'].hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8d450cac50>\n\n\n\n\n\n\ndf['collision_energy'].hist(bins=10)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8d44fc4110>\n\n\n\n\n\n\ndf['sequence_length'].hist(bins=30-7)\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8d44b09190>\n\n\n\n\n\n\n\n\n\n# Split the data into training, validation, and test\n\nfrom random import shuffle\n\ndef split_dataframe(df, \n                    unique_column, \n                    ratio_training=0.8, \n                    ratio_validation=0.1, \n                    ratio_test=0.1):\n  \"\"\"\n  This function splits the dataframe in three splits and makes sure that values\n  of `unique_column` are unique to each of the splits. This is helpful if, for \n  example, you have non-unique sequence in `unique_column` but want to ensure \n  that a sequence value is unique to one of the splits.\n  \"\"\"\n\n  assert ratio_training + ratio_validation + ratio_test == 1\n\n  unique = list(set(df[unique_column]))\n  n_unique = len(unique)\n  shuffle(unique)\n\n  train_split = int(n_unique * ratio_training)\n  val_split = int(n_unique * (ratio_training + ratio_validation))\n\n  unique_train = unique[:train_split]\n  unique_validation = unique[train_split:val_split]\n  unique_test = unique[val_split:]\n\n  assert len(unique_train) + len(unique_validation) + len(unique_test) == n_unique\n\n  df_train = df[df[unique_column].isin(unique_train)]\n  df_validation = df[df[unique_column].isin(unique_validation)]\n  df_test = df[df[unique_column].isin(unique_test)]\n\n  assert len(df_train) + len(df_validation) + len(df_test) == len(df)\n\n  return df_train, df_validation, df_test\n\ndf_train, df_validation, df_test = split_dataframe(df, unique_column='sequence_maxquant')\n\n\n# Prepare the training data\nINPUT_COLUMNS = ('sequence_integer', 'precursor_charge_onehot', 'collision_energy_aligned_normed')\nOUTPUT_COLUMN = 'intensities_raw'\n\nx_train = [np.vstack(df_train[column]) for column in INPUT_COLUMNS]\ny_train = np.vstack(df_train[OUTPUT_COLUMN])\n\nx_validation = [np.vstack(df_validation[column]) for column in INPUT_COLUMNS]\ny_validation = np.vstack(df_validation[OUTPUT_COLUMN])\n\nx_test = [np.vstack(df_test[column]) for column in INPUT_COLUMNS]\ny_test = np.vstack(df_test[OUTPUT_COLUMN])\n\n\n\n\n\n# Setup model and trainig parameters\nDIM_LATENT = 124\nDIM_EMBEDDING_IN = max(PROSIT_ALHABET.values()) + 1  # max value + zero for padding\nDIM_EMBEDDING_OUT = 32\nEPOCHS = 5\nBATCH_SIZE = 256\n\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding, Multiply\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as k\n\n# Build the model with input layers for sequence, precursor charge, and collision energy\nin_sequence = Input(shape=[x_train[0].shape[1]], name=\"in_sequence\")\nin_precursor_charge = Input(shape=[x_train[1].shape[1]], name=\"in_precursor_charge\")\nin_collision_energy = Input(shape=[x_train[2].shape[1]], name=\"in_collision_energy\")\n\nx_s = Embedding(input_dim=DIM_EMBEDDING_IN, output_dim=DIM_EMBEDDING_OUT)(in_sequence)\nx_s = GRU(DIM_LATENT)(x_s)\nx_z = Dense(DIM_LATENT)(in_precursor_charge)\nx_e = Dense(DIM_LATENT)(in_collision_energy)\nx = Multiply()([x_s, x_z, x_e])\nout_intensities = Dense(y_train.shape[1])(x)\n\nmodel = Model([in_sequence, in_precursor_charge, in_collision_energy], out_intensities)\nmodel.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n in_sequence (InputLayer)       [(None, 30)]         0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 30, 32)       704         ['in_sequence[0][0]']            \n                                                                                                  \n in_precursor_charge (InputLaye  [(None, 6)]         0           []                               \n r)                                                                                               \n                                                                                                  \n in_collision_energy (InputLaye  [(None, 1)]         0           []                               \n r)                                                                                               \n                                                                                                  \n gru (GRU)                      (None, 124)          58776       ['embedding[0][0]']              \n                                                                                                  \n dense (Dense)                  (None, 124)          868         ['in_precursor_charge[0][0]']    \n                                                                                                  \n dense_1 (Dense)                (None, 124)          248         ['in_collision_energy[0][0]']    \n                                                                                                  \n multiply (Multiply)            (None, 124)          0           ['gru[0][0]',                    \n                                                                  'dense[0][0]',                  \n                                                                  'dense_1[0][0]']                \n                                                                                                  \n dense_2 (Dense)                (None, 174)          21750       ['multiply[0][0]']               \n                                                                                                  \n==================================================================================================\nTotal params: 82,346\nTrainable params: 82,346\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n\ndef masked_spectral_distance(true, pred):\n    # This is the spectral angle implementation as used in Prosit\n    # See https://github.com/kusterlab/prosit/blob/master/prosit/losses.py#L4-L16\n    # Note, fragment ions that cannot exists (i.e. y20 for a 7mer) must have the value  -1.\n    import keras.backend as k\n\n    epsilon = k.epsilon()\n    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n    pred_norm = k.l2_normalize(true_masked, axis=-1)\n    true_norm = k.l2_normalize(pred_masked, axis=-1)\n    product = k.sum(pred_norm * true_norm, axis=1)\n    arccos = tf.acos(product)\n    return 2 * arccos / np.pi\n\nmodel.compile(optimizer='Adam', loss=masked_spectral_distance)\nhistory = model.fit(x=x_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validation, y_validation))\n\nEpoch 1/5\n2356/2356 [==============================] - 26s 8ms/step - loss: 0.4982 - val_loss: 0.4505\nEpoch 2/5\n2356/2356 [==============================] - 16s 7ms/step - loss: 0.4173 - val_loss: 0.3895\nEpoch 3/5\n2356/2356 [==============================] - 17s 7ms/step - loss: 0.3626 - val_loss: 0.3452\nEpoch 4/5\n2356/2356 [==============================] - 16s 7ms/step - loss: 0.3246 - val_loss: 0.3130\nEpoch 5/5\n2356/2356 [==============================] - 16s 7ms/step - loss: 0.2970 - val_loss: 0.2921\n\n\n\n\n\n\n# Plotting the training history \n\nimport matplotlib.pyplot as plt\n\nplt.plot(range(EPOCHS), history.history['loss'], '-', color='r', label='Training loss')\nplt.plot(range(EPOCHS), history.history['val_loss'], '--', color='r', label='Validation loss')\nplt.title(f'Training and validation loss across epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\ntest_spectral_angle = model.evaluate(x_test, y_test)\ntest_spectral_angle\n\n2376/2376 [==============================] - 8s 3ms/step - loss: 0.2921\n\n\n0.29212045669555664\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "[TODO: Add dataset introduction]\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/index.html#fragmentation",
    "href": "tutorials/index.html#fragmentation",
    "title": "Tutorials",
    "section": "Fragmentation",
    "text": "Fragmentation\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nNIST (part 1): Parsing the spectral library\n\n\nRalf Gabriels\n\n\nSep 10, 2022\n\n\n\n\nNIST (part 2): Traditional ML: Gradient boosting\n\n\nRalf Gabriels\n\n\nSep 10, 2022\n\n\n\n\nNIST (part 3): Deep learning: BiLSTM\n\n\nRalf Gabriels\n\n\nSep 10, 2022\n\n\n\n\nProsit-style GRU with ProteomeTools data\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#ion-mobility",
    "href": "tutorials/index.html#ion-mobility",
    "title": "Tutorials",
    "section": "Ion mobility",
    "text": "Ion mobility\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nTutorial - predicting CCS values for TIMS data\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/index.html#retention-time",
    "href": "tutorials/index.html#retention-time",
    "title": "Tutorials",
    "section": "Retention time",
    "text": "Retention time\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\nDate\n\n\n\n\n\n\nDLOmix Prosit\n\n\n\n\n\n\n\n\nManual embedding\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/ionmobility/index.html",
    "href": "tutorials/ionmobility/index.html",
    "title": "Ion mobility",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/ionmobility/Meier.html",
    "href": "tutorials/ionmobility/Meier.html",
    "title": "ProteomicsML.org",
    "section": "",
    "text": "Ion mobility is a technique to separate ionized analytes based on their size, shape, and physicochemical properties. Initially the techniques for ion mobility propelled the ions with an electric field through a cell with inert gas. The ions collide with the inert gas without fragmentation. Seperation is achieved by propelling the ions faster or slower in the electric field (i.e., based on their charge) and are slowed down by the collissions with the gas (i.e., based on shape and size). Trapped ion mobility (TIMS) reverses this operation by trapping the ions in an electric field and forcing them forward by collision with the gas. From any of the different ion mobility techniques you are able to derive the collisional cross section (CCS) in Angstrom squared. In this notebook you can follow a short tutorial on how to trian a Machine Learning model for the prediction of these CCS values.\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\nfrom scipy.stats import pearsonr\n\nvol_dict = {\"A\" : 88.6,\n            \"B\" : 0.0,\n            \"O\" : 0.0,\n            \"X\" : 0.0,\n            \"J\" : 0.0,\n            \"R\" : 173.4,\n            \"N\" : 114.1,\n            \"D\" : 111.1,\n            \"C\" : 108.5,\n            \"Q\" : 143.8,\n            \"E\" : 138.4,\n            \"G\" : 60.1,\n            \"H\" : 153.2,\n            \"I\" : 166.7,\n            \"L\" : 166.7,\n            \"K\" : 168.6,\n            \"M\" : 162.9,\n            \"F\" : 189.9,\n            \"P\" : 112.7,\n            \"S\" : 89.0,\n            \"T\" : 116.1,\n            \"W\" : 227.8,\n            \"Y\" : 193.6,\n            \"V\" : 140}\n\naa_to_pos = dict(zip(vol_dict.keys(),range(len(vol_dict.keys()))))\n\n\n\nRead the training data from Meier et al.\n\nccs_df = pd.read_csv(\"https://github.com/ProteomicsML/IonMobility/blob/main/datasets/Meier_IM_CCS/combined_sm.zip?raw=true\", compression=\"zip\", index_col=0)\n\nExecute the cell below to read a smaller data set from Van Puyvelde et al.. Remove all the ‚Äú#‚Äù to read this smaller data set. On for example colab it is recommended to load this smaller data set. Please do note that the description is based on the larger data set. It is expected that more complex models do not benefit at the same rate from the smaller data set (e.g., the deep learning network). Hans Vissers from Waters analyzed this traveling wave IM data:\n\n#ccs_df = pd.read_csv(\n#    \"https://raw.githubusercontent.com/ProteomicsML/IonMobility/main/datasets/VanPuyvelde_TWIMS_CCS/TWIMSpeptideCCS.tsv\",\n#    low_memory=False,\n#    sep=\"\\t\"\n#)\n\nA small summarization of the data that was just read:\n\nccs_df.describe()\n\n\n\n\n\n  \n    \n      \n      Charge\n      Mass\n      Intensity\n      Retention time\n      CCS\n    \n  \n  \n    \n      count\n      718917.000000\n      718917.000000\n      7.189170e+05\n      718917.000000\n      718917.000000\n    \n    \n      mean\n      2.376747\n      1829.771049\n      6.716163e+05\n      300.215311\n      475.545205\n    \n    \n      std\n      0.582843\n      606.256496\n      2.139819e+06\n      940.711797\n      109.083740\n    \n    \n      min\n      2.000000\n      696.428259\n      2.790800e+02\n      0.004795\n      275.418854\n    \n    \n      25%\n      2.000000\n      1361.766700\n      5.405700e+04\n      28.260000\n      392.076630\n    \n    \n      50%\n      2.000000\n      1729.834520\n      1.655000e+05\n      50.624000\n      454.656281\n    \n    \n      75%\n      3.000000\n      2189.009920\n      5.357000e+05\n      84.241000\n      534.702698\n    \n    \n      max\n      4.000000\n      4599.284130\n      2.481000e+08\n      6897.700000\n      1118.786133\n    \n  \n\n\n\n\n\nccs_df\n\n\n\n\n\n  \n    \n      \n      Modified sequence\n      Charge\n      Mass\n      Intensity\n      Retention time\n      CCS\n      PT\n    \n  \n  \n    \n      0\n      _(ac)AAAAAAAAAAGAAGGR_\n      2\n      1239.63200\n      149810.0\n      70.140\n      409.092529\n      False\n    \n    \n      1\n      _(ac)AAAAAAAAEQQSSNGPVKK_\n      2\n      1810.91734\n      21349.0\n      19.645\n      481.229248\n      True\n    \n    \n      2\n      _(ac)AAAAAAAGAAGSAAPAAAAGAPGSGGAPSGSQGVLIGDR_\n      3\n      3144.55482\n      194000.0\n      3947.700\n      772.098083\n      False\n    \n    \n      3\n      _(ac)AAAAAAAGDSDSWDADAFSVEDPVRK_\n      2\n      2634.18340\n      6416400.0\n      94.079\n      573.213196\n      False\n    \n    \n      4\n      _(ac)AAAAAAAGDSDSWDADAFSVEDPVRK_\n      3\n      2634.18340\n      5400600.0\n      94.841\n      635.000549\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      718912\n      _YYYVCQYCPAGNNM(ox)NR_\n      2\n      2087.82880\n      131230.0\n      21.753\n      461.667145\n      True\n    \n    \n      718913\n      _YYYVCQYCPAGNWANR_\n      2\n      2083.86690\n      84261.0\n      28.752\n      459.721191\n      True\n    \n    \n      718914\n      _YYYVPADFVEYEK_\n      2\n      1684.76609\n      382810.0\n      92.273\n      436.103699\n      False\n    \n    \n      718915\n      _YYYVQNVYTPVDEHVYPDHR_\n      3\n      2556.17099\n      30113.0\n      26.381\n      580.297058\n      True\n    \n    \n      718916\n      _YYYVQNVYTPVDEHVYPDHR_\n      4\n      2556.17099\n      33682.0\n      26.381\n      691.901123\n      True\n    \n  \n\n718917 rows √ó 7 columns\n\n\n\nPrepare the data to not contain any \"_\" characters or modifications in between [ ]:\n\n# Strip \"_\" from sequence\nccs_df[\"sequence\"] = ccs_df[\"Modified sequence\"].str.strip(\"_\")\n\n# Strip everything between \"()\" and \"[]\" from sequence\nccs_df[\"sequence\"] = ccs_df[\"sequence\"].str.replace(r\"[\\(\\[].*?[\\)\\]]\", \"\", regex=True)\n\nCount the occurence of amino acids, those that did not get detected; repace with 0\n\n# Apply counter to each sequence, fill NA with 0.0, make matrix from counts\nX_matrix_count = pd.DataFrame(ccs_df[\"sequence\"].apply(Counter).to_dict()).fillna(0.0).T\n\n\nX_matrix_count\n\n\n\n\n\n  \n    \n      \n      A\n      G\n      R\n      E\n      Q\n      S\n      N\n      P\n      V\n      K\n      L\n      I\n      D\n      W\n      F\n      M\n      T\n      C\n      Y\n      H\n    \n  \n  \n    \n      0\n      12.0\n      3.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      8.0\n      1.0\n      0.0\n      1.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1.0\n      2.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      17.0\n      9.0\n      1.0\n      0.0\n      1.0\n      4.0\n      0.0\n      3.0\n      1.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      9.0\n      1.0\n      1.0\n      1.0\n      0.0\n      3.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      5.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      9.0\n      1.0\n      1.0\n      1.0\n      0.0\n      3.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      5.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      718912\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      0.0\n      3.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      2.0\n      4.0\n      0.0\n    \n    \n      718913\n      2.0\n      1.0\n      1.0\n      0.0\n      1.0\n      0.0\n      2.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      2.0\n      4.0\n      0.0\n    \n    \n      718914\n      1.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      2.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      4.0\n      0.0\n    \n    \n      718915\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      2.0\n      4.0\n      0.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      5.0\n      2.0\n    \n    \n      718916\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      1.0\n      2.0\n      4.0\n      0.0\n      0.0\n      0.0\n      2.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      5.0\n      2.0\n    \n  \n\n718917 rows √ó 20 columns\n\n\n\nA fairly rudimentary technique is to use the volume of each amino acid and sum these volumes:\n\ndef to_predicted_ccs(row):\n    vol_sum = sum([vol_dict[k]*v for k,v in row.to_dict().items()])    \n    return vol_sum\n\nccs_df[\"predicted_CCS_vol_based\"] = X_matrix_count.apply(to_predicted_ccs,axis=1)\n\nLets see the results:\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\nfor c in range(2,5):\n    plt.scatter(\n        ccs_df.loc[ccs_df[\"Charge\"]==c,\"CCS\"],\n        ccs_df.loc[ccs_df[\"Charge\"]==c,\"predicted_CCS_vol_based\"],\n        alpha=set_alpha,\n        s=set_size, \n        label=\"Z=\"+str(c)\n    )\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\nplt.xlabel(\"Observed CCS (Angstrom^2)\")\nplt.xlabel(\"Predicted CCS (Angstrom^2)\")\n    \nplt.show()\n\n\n\n\nClear correlation, but seems we need to change the intercepts of each curve and make seperate predictions for each peptide charge state. In addition to these observations it seems that higher charge states have higher errors. This likely influenced by a large part by the relation between higher charge states and longer peptides. These longer peptides can deviate more from each other in terms of structures (and CCS). Instead of spending more time on this, lets have a look at a more ML-based approach.\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport random\n\nIn this section we will fit a linear regression model. This model is only able to fit a linear function between the features (sequence) and target (CCS). This linear model can be expressed as the following equation:\n$ Y = _0 + _1 X$\nWhere \\(Y\\) is a vector (/list) of all CCS values and X a matrix (/2-dimensional list) of all the amino acids counts. The intercept and weights of each features are learned so the predicted value (\\(\\hat{Y}\\)) is close to the observed outcome (\\(Y\\)). What is considered close and how this closeness between predictions and observations are minimized is not further discussed here. However, there is a rich amount of information available on the internet (e.g., https://www.coursera.org/learn/machine-learning).\nFirst, we will split the matrix into 90% training peptides and 10% testing peptides. These testing peptides are very valuable in estimating model performance. Since the model has not seen these sequences before it cannot overfit on these particular examples.\n\n# Get all the index identifiers\nall_idx = list(X_matrix_count.index)\nrandom.seed(42)\n\n# Shuffle the index identifiers so we can randomly split them in a testing and training set\nrandom.shuffle(all_idx)\n\n# Select 90 % for training and the remaining 10 % for testing\ntrain_idx = all_idx[0:int(len(all_idx)*0.9)]\ntest_idx = all_idx[int(len(all_idx)*0.9):]\n\n# Get the train and test indices and point to new variables\nccs_df_train = ccs_df.loc[train_idx,:]\nccs_df_test = ccs_df.loc[test_idx,:]\n\n# Also for the feature matrix get the train and test indices\nX_matrix_count_train = X_matrix_count.loc[train_idx,:]\nX_matrix_count_test = X_matrix_count.loc[test_idx,:]\n\nNow lets start training the models. Although we could encode the charge as a feature here we separate all models to counter any charge to composition specific patterns.\n\n# Initialize a model object\nlinear_model_z2 = LinearRegression()\n\n# Fit the initialized model object to our training data (only charge 2)\nlinear_model_z2.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==2,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==2,\"CCS\"]\n)\n\n# Repeat for the other two charge states\nlinear_model_z3 = LinearRegression()\nlinear_model_z3.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==3,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==3,\"CCS\"]\n)\n\nlinear_model_z4 = LinearRegression()\nlinear_model_z4.fit(\n    X=X_matrix_count_train.loc[ccs_df_train[\"Charge\"]==4,:],\n    y=ccs_df_train.loc[ccs_df_train[\"Charge\"]==4,\"CCS\"]\n)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nNow we can have a look at the coefficients ($_1) learned. These should be highly correlated with the previous experimentally determined volumetric observations for each amino acid:\n\n# Scatter plot the coefficients of each amino acid against their experimentally determined volumes\nplt.scatter(\n    linear_model_z2.coef_,\n    [vol_dict[v] for v in X_matrix_count.columns]\n)\n\n# Plot a diagonal line we expect the points to be on\nplt.plot(\n    [6.0,26.0],\n    [60.0,260],\n    c=\"grey\",\n    zorder=0\n)\n\n# Annotate each point with their respective amino acids\nfor v,x,y in zip(X_matrix_count.columns,\n                 linear_model_z2.coef_,\n                 [vol_dict[v] for v in X_matrix_count.columns]):\n    \n    plt.annotate(v,(x+0.1,y+5.0))\n    \nplt.show()\n\n\n\n\nObservations are very similar. There are differences that could be cause by a multitude of reasons. For example, the difference between volumetric observations in the CCS cell is different or being part of a polypeptide chain changes the volume of the amino acid.\nNext we will plot the predictions of the test set and compare them with observational data. Note that we apply each charge model seperately.\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=2)\nplt.scatter(\n    linear_model_z2.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=3)\nplt.scatter(\n    linear_model_z3.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\n# Scatter plot the observations on the test set against the predictions on the same set (z=4)\nplt.scatter(\n    linear_model_z4.predict(X=X_matrix_count_test.loc[ccs_df[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\n# Add a legend for the charge states\nlegend = plt.legend()\n\n# Make sure the legend labels are visible and big enough\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n    \n# Get the predictions and calculate performance metrics\npredictions = linear_model_z2.predict(X_matrix_count_test.loc[ccs_df[\"Charge\"]==3,:])\nmare = round(sum((abs(predictions-ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])/ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])/ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"Linear model - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}% (z3 model for z3 observations)\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nIt is clear that the predictions and observations are on the diagonal. This means that they are very similar. However, there are still some differences between observations and predictions.\nIn the previous example we trained models for charge state seperately. This is slightly inconvenient and other charge states might still be able to provide useful training examples. As long as the model corrects for the right charge state of course. In the next example we add charge state to the feature matrix. The linear model should be (partially‚Ä¶) able to account for the charge states of peptides.\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Fit the linear model, but this time with the charge as a feature\nlinear_model = LinearRegression()\n\nlinear_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=1,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=1,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    linear_model.predict(X=X_matrix_count_charge_test.loc[ccs_df[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = linear_model.predict(X=X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"Linear model - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n    \nplt.show()\n\n\n\n\nWith this model we are capable to predict CCS values for all three charge states (maybe more; be careful with extrapolation). However, it also shows that both z3 and z4 are not optimally predicted. Especially z4 we can probably draw a line manually that provides better performance than the current model. The incapability of the model to correctly predict some of these values is largely due to the linear algorithm. With this algorithm we can only fit ‚Äúsimple‚Äù linear relations, but more complex relations are not modeled correctly. In the next section we will fit a non-linear model that is able to capture these complex relations better. However, keep in mind that more complex models are usually also able to overfit data better, resulting in poorer generalization performance.\n\n\n\nIn this section we will fit a random forest (RF) regression model. We hope to fit some of the non-linear relations present in the data. The RF algorithm fits multiple decision trees, but what makes these trees different is the random selection of instances (peptides) and/or features (amino acid count). The predictions between the forest of trees can be averaged to obtain a single prediction per peptide (instead of multiple for the same peptide). Later we will see that the algorithm might actually not be suitable for fitting this type of data.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Initialize a RF object, note the hyperparameters that the model will follow\nrf_model = RandomForestRegressor(\n                max_depth=20,\n                n_estimators=50,\n                n_jobs=-1\n)\n\n# Fit the RF model\nrf_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nRandomForestRegressor(max_depth=20, n_estimators=50, n_jobs=-1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=20, n_estimators=50, n_jobs=-1)\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==2,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    rf_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = rf_model.predict(X=X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"RF - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nAs can be observed the problem with z=4 splitting up is gone, probably due to the capability of RF to fit non-linear relations. However, we see quite a large deviation on the diagonal. One of the major causes of this problem is the exclusion of amino acid counts for the decision trees. Although this is fundamental to the inner workings of RF, it means that we cannot take the excluded amino acids into account and these values are likely to be replaced by average expected volume to other (non-excluded) amino acids. RF performs very well when features correlate, and predictions are not fully dependent on the inclusion of all features. Next we will look at a decision tree algorithm (XGBoost) that does not rely on the exclusion of features.\nPS note that you might be able to fit a much better model by using a much larger number of trees, but overall the problem largely remains, and it is better to choose an algorithm that respects/fits your data best.\n\n\n\nIn this section we will fit a XGBoost regression model. This algorithm works by training a sequence of underfitted models. Each model in the sequence receives the output of the previous decision tree models. This combination of trees allows to fit the data well without greatly overfitting it.\n\nfrom xgboost import XGBRegressor\n\n\n# Make a new copy of feature matrix and add charge as a feature\nX_matrix_count_charge_train = X_matrix_count_train.copy()\nX_matrix_count_charge_train[\"charge\"] = ccs_df_train[\"Charge\"]\n\nX_matrix_count_charge_test = X_matrix_count_test.copy()\nX_matrix_count_charge_test[\"charge\"] = ccs_df_test[\"Charge\"]\n\n\n# Initialize the XGB object\nxgb_model = XGBRegressor(\n                max_depth=12,\n                n_estimators=250\n)\n\n# Fit the XGB model\nxgb_model.fit(\n    X=X_matrix_count_charge_train,\n    y=ccs_df_train.loc[:,\"CCS\"]\n)\n\nXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=12, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=250, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=12, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=250, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, ...)\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==2,:]),\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\")\n\nplt.scatter(\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==3,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    xgb_model.predict(X=X_matrix_count_charge_test.loc[ccs_df_test[\"Charge\"]==4,:]),\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n    \n# Get the predictions and calculate performance metrics\npredictions = xgb_model.predict(X_matrix_count_charge_test)\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"XGBoost - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\n\n\n\nThe deviation on the diagonal has been decreased significantly. But‚Ä¶ A decision tree based algorithm is usually not the best for a regression model. Since the target data is continuous a model that can respect this structure is likely to perform better. Furthermore, up till now we simply counted amino acids, but structure is important. So to get the most out of the data we need to use the exact positions of amino acids.\nAlso‚Ä¶ We have a lot of data it makes sense to use deep learning (DL). DL models are usually capable of learning more complex relations than traditional algorithms. Furthormore, for traditional ML algorithms we usually need to engineer features, while DL can usually work directly from raw data. DL is able to construct its own features.\n\nfrom tensorflow.keras.layers import Dense, concatenate, Input, Bidirectional, LSTM\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\nAs mentioned before, we want to use features that can also tell us something about the potential structure of the peptide. This means we need to take the sequence of the peptide into account and not just the amino acid counts. For this we will use a ‚Äòone-hot encoding‚Äô, in this matrix each position in the peptide are the columns (number of columns equals the length of the peptide) and each amino acid per position has its own row (for the standard amino acids this is 20). So as a result we create a matrix that is the length of the peptide by the amount of unique amino acids in the whole data set. For each position we indicate the presence with a ‚Äò1‚Äô and absence with ‚Äò0‚Äô. As a result the sum of each columnn is ‚Äò1‚Äô and the sum of the whole matrix equals the length of the peptide.\n\ndef aa_seq_to_one_hot(seq,padding_length=60):\n    # Although padding is not needed for an LSTM, we might need it if we for example apply a CNN\n    # Calculate how much padding is needed\n    seq_len = len(seq)\n    if seq_len > padding_length:\n        seq = seq[0:padding_length]\n        seq_len = len(seq)\n\n    # Add padding for peptides that are too short\n    padding = \"\".join([\"X\"] * (padding_length - len(seq)))\n    seq = seq + padding\n\n    # Initialize all feature matrix\n    matrix_hc = np.zeros(\n        (len(aa_to_pos.keys()), len(seq)), dtype=np.int8)\n    \n    # Fill the one-hot matrix, when we encounter an 'X' it should be the end of the sequence\n    for idx,aa in enumerate(seq):\n        if aa == \"X\":\n            break\n        matrix_hc[aa_to_pos[aa],idx] = 1\n    \n    return matrix_hc\n\n\n# Calculate the one-hot matrices and stack them\n# Result is a 3D matrix where the first dimension is each peptide, and then the last two dims are the one-hot matrix\none_hot_encoded_train = np.stack(ccs_df_train[\"sequence\"].apply(aa_seq_to_one_hot).values)\none_hot_encoded_test = np.stack(ccs_df_test[\"sequence\"].apply(aa_seq_to_one_hot).values)\n\n\nif len(ccs_df.index) < 1e4:\n    epochs = 100\n    num_lstm = 12\n    batch_size = 128\nelse:\n    batch_size = 1024\n    epochs = 10\n    num_lstm = 64\n\nbatch_size = 128\nv_split    = 0.1\noptimizer  = \"adam\"\nloss       = \"mean_squared_error\"\n\n# The architecture chosen consists of two inputs: (1) the one-hot matrix and (2) the charge\n# The first part is a biderectional LSTM (a), in paralel we have dense layers containing the charge (b)\n# Both a and b are concatenated to go through several dense layers (c)\ninput_a = Input(shape=(None, one_hot_encoded_train.shape[2]))\na = Bidirectional(LSTM(num_lstm,return_sequences=True))(input_a)\na = Bidirectional(LSTM(num_lstm))(a)\na = Model(inputs=input_a, outputs=a)\n\ninput_b = Input(shape=(1,))\nb = Dense(5, activation=\"relu\")(input_b)\nb = Model(inputs=input_b, outputs=b)\n\nc = concatenate([a.output, b.output],axis=-1)\n\nc = Dense(64, activation=\"relu\")(c)\nc = Dense(32, activation=\"relu\")(c)\nc = Dense(1, activation=\"relu\")(c)\n\n# Create the model with specified inputs and outputs\nmodel = Model(inputs=[a.input, b.input], outputs=c)\n\nmodel.compile(optimizer=optimizer, loss=loss)\n\n# Fit the model on the training data\nhistory = model.fit(\n            (one_hot_encoded_train,ccs_df_train.loc[:,\"Charge\"]),\n            ccs_df_train.loc[:,\"CCS\"],\n            epochs=epochs, \n            batch_size=batch_size,\n            validation_split=v_split\n)\n\nEpoch 1/10\n4550/4550 [==============================] - 108s 23ms/step - loss: 5536.7256 - val_loss: 467.1984\nEpoch 2/10\n4550/4550 [==============================] - 102s 22ms/step - loss: 436.4136 - val_loss: 403.9187\nEpoch 3/10\n4550/4550 [==============================] - 100s 22ms/step - loss: 395.7551 - val_loss: 384.5470\nEpoch 4/10\n4550/4550 [==============================] - 103s 23ms/step - loss: 376.4315 - val_loss: 382.0145\nEpoch 5/10\n4550/4550 [==============================] - 102s 23ms/step - loss: 364.8819 - val_loss: 395.8338\nEpoch 6/10\n4550/4550 [==============================] - 106s 23ms/step - loss: 357.4092 - val_loss: 355.8185\nEpoch 7/10\n4550/4550 [==============================] - 104s 23ms/step - loss: 342.5216 - val_loss: 312.9571\nEpoch 8/10\n4550/4550 [==============================] - 104s 23ms/step - loss: 275.4517 - val_loss: 274.8963\nEpoch 9/10\n4550/4550 [==============================] - 110s 24ms/step - loss: 253.2955 - val_loss: 252.0118\nEpoch 10/10\n4550/4550 [==============================] - 105s 23ms/step - loss: 240.6064 - val_loss: 260.1613\n\n\n\n# Predict CCS values test set\nccs_df_test[\"LSTM_predictions\"] = model.predict((one_hot_encoded_test,ccs_df_test.loc[:,\"Charge\"]))\n\n2247/2247 [==============================] - 20s 8ms/step\n\n\n\nif len(ccs_df.index) < 1e4:\n    set_alpha = 0.2\n    set_size = 3\nelse:\n    set_alpha = 0.05\n    set_size = 1\n\n# Scatter plot the observations on the test set against the predictions on the same set\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==2,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=2\"\n)\n\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==3,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=3\"\n)\n\nplt.scatter(\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"CCS\"],\n    ccs_df_test.loc[ccs_df_test[\"Charge\"]==4,\"LSTM_predictions\"],\n    alpha=set_alpha,\n    s=set_size,\n    label=\"Z=4\"\n)\n\n# Plot a diagonal the points should be one\nplt.plot([300,1100],[300,1100],c=\"grey\")\n\nlegend = plt.legend()\n\nfor lh in legend.legendHandles:\n    lh.set_sizes([25])\n    lh.set_alpha(1)\n\n# Get the predictions and calculate performance metrics\npredictions = ccs_df_test[\"LSTM_predictions\"]\nmare = round(sum((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100)/len(predictions),3)\npcc = round(pearsonr(predictions,ccs_df_test.loc[:,\"CCS\"])[0],3)\nperc_95 = round(np.percentile((abs(predictions-ccs_df_test.loc[:,\"CCS\"])/ccs_df_test.loc[:,\"CCS\"])*100,95)*2,2)\n\nplt.title(f\"LSTM - PCC: {pcc} - MARE: {mare}% - 95th percentile: {perc_95}%\")\n\nax = plt.gca()\nax.set_aspect('equal')\n\nplt.xlabel(\"Observed CCS (^2)\")\nplt.ylabel(\"Predicted CCS (^2)\")\n\nplt.show()\n\n\n\n\nIt is clear that the performance of this model is much better. But‚Ä¶ Performance can be improved a lot more by for example tuning hyperparameters like the network architecture or number of epochs.\nHope you enjoyed this tutorial! Feel free to edit it and make a pull request!\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/retentiontime/DLomix Prosit.html",
    "href": "tutorials/retentiontime/DLomix Prosit.html",
    "title": "ProteomicsML.org",
    "section": "",
    "text": "DLOmix Prosit\n\n %%capture\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install dlomix\n!pip install numpy\n!pip install matplotlib\n!pip install dlomix\n!pip install requests\n\n\n# Import and normalize/standarize data\nimport pandas as pd\nimport numpy as np\n# Import and normalize the data\ndata = pd.read_csv('https://raw.githubusercontent.com/ProteomicsML/RetentionTime/main/datasets/ProteomeTools/Small.csv')\n\n# shuffle and split dataset into internal (80%) and external (20%) datasets\ndata = data.sample(frac=1)\ntest_data = data[int(len(data)*0.8):]\ndata = data[:int(len(data)*0.8)]\n\n\n# Split the internal dataset into training and validation\n# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\nunique_sequences = list(set(data['Sequence']))\n# Shuffle the data to ensure unbiased data splitting\nfrom random import shuffle\nshuffle(unique_sequences)\n# Split sequence 80-10-10 training, validation and testing split\ntrain = unique_sequences[0:int(len(unique_sequences) * 0.8)]\nvalidation = unique_sequences[int(len(unique_sequences) * 0.8):]\n# Transfer the sequence split into data split\ntrain = data[data['Sequence'].isin(train)]\nvalidation = data[data['Sequence'].isin(validation)]\nprint('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n# Here we use test as an external dataset unlike the one used for training.\n\nTraining data points: 63819   Validation data points: 16181   Testing data points: 20000\n\n\n\nnormalize = True\nif normalize:\n  # Normalize\n  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\nelse:\n  # Standardize\n  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n# Setup parameters\nsequence_length = 30\nbatch_size = 64\nepochs=10\n\n\n# Setup data \nfrom dlomix.data import RetentionTimeDataset\ntrain_input = RetentionTimeDataset(data_source=tuple([np.array(train['Sequence']), np.array(train['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\nval_input = RetentionTimeDataset(data_source=tuple([np.array(validation['Sequence']), np.array(validation['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\ntest_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['Sequence']), np.array(test_data['Retention time'])]), \n                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n\n# Setup PROSIT model from DLOmix\nfrom dlomix.models.prosit import PrositRetentionTimePredictor\nmodel = PrositRetentionTimePredictor(seq_length=sequence_length)\nmodel.build((None, sequence_length))\nmodel.summary()\n\nModel: \"prosit_retention_time_predictor\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n string_lookup (StringLookup  multiple                 0         \n )                                                               \n                                                                 \n embedding (Embedding)       multiple                  352       \n                                                                 \n sequential (Sequential)     (None, 30, 512)           1996800   \n                                                                 \n attention_layer (AttentionL  multiple                 542       \n ayer)                                                           \n                                                                 \n sequential_1 (Sequential)   (None, 512)               262656    \n                                                                 \n dense_1 (Dense)             multiple                  513       \n                                                                 \n=================================================================\nTotal params: 2,260,863\nTrainable params: 2,260,863\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom dlomix.eval.rt_eval import TimeDeltaMetric\nimport tensorflow as tf\n# Compiling the keras model with loss function, metrics and optimizer\nmodel.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n# Train the model\nhistory = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)\n\nEpoch 1/10\n973/973 [==============================] - 38s 22ms/step - loss: 0.4840 - mae: 0.1073 - timedelta: 0.1060 - val_loss: 0.0050 - val_mae: 0.0564 - val_timedelta: 0.0596\nEpoch 2/10\n973/973 [==============================] - 22s 22ms/step - loss: 0.0046 - mae: 0.0475 - timedelta: 0.0486 - val_loss: 0.0029 - val_mae: 0.0347 - val_timedelta: 0.0385\nEpoch 3/10\n973/973 [==============================] - 20s 21ms/step - loss: 0.0039 - mae: 0.0430 - timedelta: 0.0433 - val_loss: 0.0028 - val_mae: 0.0354 - val_timedelta: 0.0380\nEpoch 4/10\n973/973 [==============================] - 21s 22ms/step - loss: 0.0034 - mae: 0.0394 - timedelta: 0.0396 - val_loss: 0.0038 - val_mae: 0.0408 - val_timedelta: 0.0451\nEpoch 5/10\n973/973 [==============================] - 21s 22ms/step - loss: 0.0031 - mae: 0.0372 - timedelta: 0.0373 - val_loss: 0.0025 - val_mae: 0.0324 - val_timedelta: 0.0348\nEpoch 6/10\n973/973 [==============================] - 22s 23ms/step - loss: 0.0029 - mae: 0.0355 - timedelta: 0.0366 - val_loss: 0.0025 - val_mae: 0.0291 - val_timedelta: 0.0326\nEpoch 7/10\n973/973 [==============================] - 21s 22ms/step - loss: 0.0028 - mae: 0.0347 - timedelta: 0.0358 - val_loss: 0.0025 - val_mae: 0.0300 - val_timedelta: 0.0329\nEpoch 8/10\n973/973 [==============================] - 21s 22ms/step - loss: 0.0464 - mae: 0.0889 - timedelta: 0.0862 - val_loss: 0.0441 - val_mae: 0.1755 - val_timedelta: 0.1689\nEpoch 9/10\n973/973 [==============================] - 21s 22ms/step - loss: 0.0207 - mae: 0.1059 - timedelta: 0.1066 - val_loss: 0.0066 - val_mae: 0.0606 - val_timedelta: 0.0567\nEpoch 10/10\n973/973 [==============================] - 21s 21ms/step - loss: 0.0148 - mae: 0.0800 - timedelta: 0.0834 - val_loss: 0.0058 - val_mae: 0.0557 - val_timedelta: 0.0536\n\n\n\nimport matplotlib.pyplot as plt\n# Plotting the training history \nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.plot(range(epochs), history.history['val_loss'], '--', color='r', label='Validation loss')\nplt.title(f'Training and validation loss across epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Initially we trained on just one gradient, to transfer this model to external datasets, \n# we refine the model by using the model we just trained as a pre-trained model, and then further train it with the test/external dataset\nhistory = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\n# The model can now be used for other datasets with the same gradient set-up\n# We then plot the history of this model, and see the initial performance is much better, \n# as the model already has some gradient agnostic knowledge, and it simply has to learn the new gradients\nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.title(f'Training and validation loss of the refined model')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nEpoch 1/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0148 - mae: 0.0789 - timedelta: 0.0762\nEpoch 2/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0148 - mae: 0.0767 - timedelta: 0.0743\nEpoch 3/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0148 - mae: 0.0764 - timedelta: 0.0763\nEpoch 4/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0148 - mae: 0.0760 - timedelta: 0.0679\nEpoch 5/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0148 - mae: 0.0743 - timedelta: 0.0671\nEpoch 6/10\n306/306 [==============================] - 6s 20ms/step - loss: 0.0148 - mae: 0.0751 - timedelta: 0.0684\nEpoch 7/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0144 - mae: 0.0730 - timedelta: 0.0710\nEpoch 8/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0142 - mae: 0.0728 - timedelta: 0.0654\nEpoch 9/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0145 - mae: 0.0729 - timedelta: 0.0701\nEpoch 10/10\n306/306 [==============================] - 6s 19ms/step - loss: 0.0143 - mae: 0.0736 - timedelta: 0.0644\n\n\n\n\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/retentiontime/index.html",
    "href": "tutorials/retentiontime/index.html",
    "title": "Retention time",
    "section": "",
    "text": "This work is licensed under a Creative Commons CC-BY 4.0 license."
  },
  {
    "objectID": "tutorials/retentiontime/Manual Embedding.html",
    "href": "tutorials/retentiontime/Manual Embedding.html",
    "title": "ProteomicsML.org",
    "section": "",
    "text": "Manual embedding\n\n %%capture\n!pip install pandas\n!pip install sklearn\n!pip install tensorflow\n!pip install dlomix\n!pip install numpy\n!pip install matplotlib\n!pip install requests\n\n\n# Import and normalize/standarize data\nimport pandas as pd\nimport numpy as np\n# Import and normalize the data\ndata = pd.read_csv('https://raw.githubusercontent.com/ProteomicsML/RetentionTime/main/datasets/ProteomeTools/Small.csv')\n\n# shuffle and split dataset into internal (80%) and external (20%) datasets\ndata = data.sample(frac=1)\ntest_data = data[int(len(data)*0.8):]\ndata = data[:int(len(data)*0.8)]\n\n\n# Split the internal dataset into training and validation\n# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\nunique_sequences = list(set(data['Sequence']))\n# Shuffle the data to ensure unbiased data splitting\nfrom random import shuffle\nshuffle(unique_sequences)\n# Split sequence 80-10-10 training, validation and testing split\ntrain = unique_sequences[0:int(len(unique_sequences) * 0.8)]\nvalidation = unique_sequences[int(len(unique_sequences) * 0.8):]\n# Transfer the sequence split into data split\ntrain = data[data['Sequence'].isin(train)]\nvalidation = data[data['Sequence'].isin(validation)]\nprint('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n# Here we use test as an external dataset unlike the one used for training.\n\nTraining data points: 64613   Validation data points: 15387   Testing data points: 20000\n\n\n\nnormalize = True\nif normalize:\n  # Normalize\n  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\nelse:\n  # Standardize\n  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])\n\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n\n\n\n# Setup parameters\nsequence_length = 30\nbatch_size = 64\nepochs=10\n\n\n# Manual sequence embedding\n# Remove sequences longer than our maximum sequence length\ntrain = train[train['Sequence'].str.len()<=sequence_length]\nvalidation = validation[validation['Sequence'].str.len()<=sequence_length]\ntest_data = test_data[test_data['Sequence'].str.len()<=sequence_length]\n\n# Create an alphabet to convert from string to numeric\nAA_alphabet = {\"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10, \"M\": 11, \"N\": 12, \"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \"W\": 19, \"Y\": 20}\n# Convert sequences from string to numberic\nembedded_sequences_train = [[AA_alphabet[g] for g in f] for f in train['Sequence']]\nembedded_sequences_validation = [[AA_alphabet[g] for g in f] for f in validation['Sequence']]\nembedded_sequences_test = [[AA_alphabet[g] for g in f] for f in test_data['Sequence']]\n\n# Make sure every sequence is the same length\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nembedded_sequences_train = pad_sequences(sequences=embedded_sequences_train, maxlen=sequence_length)\nembedded_sequences_validation = pad_sequences(sequences=embedded_sequences_validation, maxlen=sequence_length)\nembedded_sequences_test = pad_sequences(sequences=embedded_sequences_test, maxlen=sequence_length)\n\n\n# Import the needed layers and tensorflow model requirements\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Bidirectional, Dropout\nfrom tensorflow.keras.models import Model\n\n# Now we create a neural network with the manual embedding\n# Input layer with the dimensionality of our embedded sequences\ninputs = Input(shape=(sequence_length,), name='Input')\n# Embed the sequnces in a 20 x 8 matrix\ninput_embedding = Embedding(input_dim=len(AA_alphabet)+2, output_dim=8, name='Sequence_Embedding')(inputs)\n# Throw the embedding into a Bi-directional LSTM layer with return sequence to get outputs for each input (https://keras.io/api/layers/recurrent_layers/lstm/)\nx = Bidirectional(LSTM(32, return_sequences=True), name='Bi_LSTM_1')(input_embedding)\n# Adding some dropout for regulatization purposes\nx = Dropout(0.25, name='LSTM_Dropout')(x)\n# Adding another Bi-directional LSTM layer without return Sequence\nx = Bidirectional(LSTM(32), name='Bi_LSTM_2')(x)\n# Create an output layer with a single node and a linear regression activation function\noutput = Dense(1, activation=\"linear\", name='Output')(x)\n# Create the model from sequence input to linear output\nmodel = Model(inputs, output)\n# Print the model paramters\nmodel.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Input (InputLayer)          [(None, 30)]              0         \n                                                                 \n Sequence_Embedding (Embeddi  (None, 30, 8)            176       \n ng)                                                             \n                                                                 \n Bi_LSTM_1 (Bidirectional)   (None, 30, 64)            10496     \n                                                                 \n LSTM_Dropout (Dropout)      (None, 30, 64)            0         \n                                                                 \n Bi_LSTM_2 (Bidirectional)   (None, 64)                24832     \n                                                                 \n Output (Dense)              (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 35,569\nTrainable params: 35,569\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nimport tensorflow as tf\n# Compiling the keras model with loss function, metrics and optimizer\nmodel.compile(loss='mse', metrics=['mae'], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n# Train the model\nhistory = model.fit(x=embedded_sequences_train, y=train['Retention time'], epochs=epochs, \n                    batch_size=batch_size, validation_data=(embedded_sequences_validation, validation['Retention time']))\n\nEpoch 1/10\n983/983 [==============================] - 26s 19ms/step - loss: 0.0073 - mae: 0.0556 - val_loss: 0.0036 - val_mae: 0.0406\nEpoch 2/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0035 - mae: 0.0395 - val_loss: 0.0028 - val_mae: 0.0340\nEpoch 3/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0029 - mae: 0.0355 - val_loss: 0.0034 - val_mae: 0.0361\nEpoch 4/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0026 - mae: 0.0322 - val_loss: 0.0026 - val_mae: 0.0317\nEpoch 5/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0025 - mae: 0.0313 - val_loss: 0.0024 - val_mae: 0.0280\nEpoch 6/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0025 - val_mae: 0.0313\nEpoch 7/10\n983/983 [==============================] - 14s 14ms/step - loss: 0.0023 - mae: 0.0292 - val_loss: 0.0024 - val_mae: 0.0277\nEpoch 8/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0022 - mae: 0.0288 - val_loss: 0.0023 - val_mae: 0.0296\nEpoch 9/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0021 - mae: 0.0281 - val_loss: 0.0021 - val_mae: 0.0271\nEpoch 10/10\n983/983 [==============================] - 13s 13ms/step - loss: 0.0021 - mae: 0.0276 - val_loss: 0.0025 - val_mae: 0.0347\n\n\n\nimport matplotlib.pyplot as plt\n# Plotting the training history \nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.plot(range(epochs), history.history['val_loss'], '--', color='r', label='Validation loss')\nplt.title(f'Training and validation loss across epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Initially we trained on just one gradient, to transfer this model to external datasets, \n# we refine the model by using the model we just trained as a pre-trained model, and then further train it with the test/external dataset\nhistory = model.fit(x=embedded_sequences_test, y=test_data['Retention time'], epochs=epochs, batch_size=batch_size)\n# The model can now be used for other datasets with the same gradient set-up\n# We then plot the history of this model, and see the initial performance is much better, \n# as the model already has some gradient agnostic knowledge, and it simply has to learn the new gradients\nplt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\nplt.title(f'Training and validation loss of the refined model')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nEpoch 1/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0021 - mae: 0.0285\nEpoch 2/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0020 - mae: 0.0278\nEpoch 3/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0019 - mae: 0.0270\nEpoch 4/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 5/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 6/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0264\nEpoch 7/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0261\nEpoch 8/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0018 - mae: 0.0267\nEpoch 9/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0253\nEpoch 10/10\n306/306 [==============================] - 4s 12ms/step - loss: 0.0017 - mae: 0.0262\n\n\n\n\n\n\n\n\n\nThis work is licensed under a Creative Commons CC-BY 4.0 license."
  }
]