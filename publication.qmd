---
title: "ProteomicsML: An Online Platform for Community-Curated Datasets and Tutorials for Machine Learning in Proteomics"
date: 10/4/2022
author:
 - name: Tobias G. Rehfeldt*
   orcid: 0000-0002-1190-9485
 - name: Ralf Gabriels*
   orcid: 0000-0002-1679-1711
 - name: Robbin Bouwmeester*
   orcid: 0000-0001-6807-7029
 - name: Siegfried Gessulat
   orcid: 0000-0001-5530-0674
 - name: Benjamin A. Neely
   orcid: 0000-0001-6120-7695
 - name: Magnus Palmblad
   orcid: 0000-0002-5865-8994
 - name: Yasset Perez-Riverol
   orcid: 0000-0001-6579-6941
 - name: Tobias Schmidt
   orcid: 0000-0002-1883-6514
 - name: Juan Antonio Vizcaíno^§^
   orcid: 0000-0002-3905-4335
 - name: Eric W. Deutsch^§^
   orcid: 0000-0001-8732-0928

abstract: Dataset acquisition and curation are often the hardest and most time-consuming parts of a machine learning endeavor. This is especially true for proteomics-based LC-IM-MS datasets, due to the high-throughput data structure with high levels of noise and complexity between raw and machine learning-ready formats. While predictive proteomics is a field on the rise, when predicting peptide behavior in LC-IM-MS setups, each lab often uses unique and complex data processing pipelines in order to maximize performance, at the cost of accessibility and reproducibility. For this reason we introduce ProteomicsML, an online resource for proteomics-based datasets and tutorials across most of the currently explored physico-chemical peptide properties. This community-driven resource makes it simple to access data in easy-to-process formats, and contains easy-to-follow tutorials that allow new users to interact with even the most advanced algorithms in the field. ProteomicsML provides datasets that are useful for comparing state-of-the-art (SOTA) machine learning algorithms, as well as providing introductory material for teachers and newcomers to the field alike. The platform is freely available on [proteomicsml.org](https://www.proteomicsml.org/) and we welcome the entire proteomics community to contribute to the project at [github.com/proteomicsml](https://github.com/proteomicsml/).

---

*Manuscript submitted on October 4, 2022.*

## Introduction

Computational predictions of analyte behavior in the context of mass spectrometry (MS) data have been explored for nearly five decades, with early rudimentary predictions dating back to 1983 [@Von_Heijne1983-cu]. With the rise of technology and computational power, machine learning (ML) approaches were introduced into the field of proteomics in 1998 [@Nielsen1999-ej] and ML-based models quickly overtook human accuracy. Since then, dozens of articles have described efforts to train models for a range of physio-chemical properties associated with the field of high-throughput proteomics, as reviewed by Neely et al. (submitted, this issue). While many efforts are still in the realm of basic exploratory research, ML approaches are increasingly being incorporated into mainstream tools and standalone predictive resources [@Gessulat2019-rt; @Bouwmeester2021-cf; @Wen2020-cp; @Meyer2021-jm].

When training any ML model it is key to have suitable training and evaluation datasets. Likewise, in many fields of research where ML is applied, it is common to have a range of educational datasets, such as MNIST or IRIS, allowing newcomers to the field to easily learn common ML methodologies. Likewise, state-of-the-art (SOTA) models can use benchmarking datasets such as ImageNet or those available on the UCI Machine Learning Repository to compare their predictive capabilities. Similar to how the length of iris petals or the numbers of survivors of the Titanic have been modeled close to 50000 times ([kaggle.com](https://www.kaggle.com/competitions/titanic)), we seek to define proteomics datasets that can provide an entry point for ML modeling.

Although there have been numerous efforts to explore the predictive capabilities of models, there are several barriers that limit widespread adoption in the field of predictive proteomics. First, there are substantial difficulties in accessing datasets in a suitable form for ML applications. A substantial amount of effort is required to prepare raw proteomics datasets into a usable form, requiring expertise in proteomics data processing and intimate knowledge of the many post-processing methods available. Recently, tools such as ppx [@Fondrie2021-nb] and MS2AI [@Rehfeldt2021-iw] were created to facilitate this process, but they are still limited to certain use cases due to the complex nature of LC-IM-MS data.

Second, while some ML-ready datasets are available on platforms such as Kaggle ([kaggle.com/datasets?search=proteomics](https://www.kaggle.com/datasets?search=proteomics)) or in supplementary tables of publications, they are often difficult to find and lack long term maintenance and support post-publication. While there is no formal dataset consensus in the field, there are certain datasets that are often used for training such as ProteomeTools [@Zolg2017-ys]. Nevertheless, there are no widely used datasets used to compare the performance of tools developed by different researchers, making it difficult for new algorithms to be evaluated and compared to older tools. This issue is only further exacerbated by individual groups relying on different pre- and post-processing protocols, such as normalization of measurements and re-scoring of PSMs [@Gessulat2019-rt].

As an outcome of the 2022 Lorentz Center workshop on Proteomics and Machine Learning, we have created a web platform to facilitate the application of ML approaches to the field of MS-based proteomics. The resource is intended to provide a central focal point for curating and disseminating datasets that are ready to use for ML research, providing benchmark datasets for comparing different approaches, and encouraging new entrants into the field through expert-driven tutorials and other teaching materials.

Here we describe how the resource has been developed using commonly available tools and with future ease of maintenance in mind. We provide a brief overview of the datasets that are currently available at the resource and how it can be expanded with more data. We also describe the initial set of tutorials that can be used as an introduction to the field of ML in proteomics.


## The resource

The primary entry point for the resource is the [ProteomicsML.org](https://www.proteomicsml.org) website. It provides pages for general introductory datasets that are pre-processed and ready for training or evaluation, and pages for teaching resources and tutorials for those new to ML in proteomics. The code base for the website is maintained via a GitHub repository, and therefore is easy to maintain and amenable to outside contributions from the field. We also collaborated with PRIDE to host larger datasets on a dedicated FTP server for ProteomicsML.

A key goal of ProteomicsML is to grow together with the field, which is why we provide experts with a contributing guide on how to upload datasets and tutorials for specific ML workflows or algorithms. After curation by the maintainers, contributions are automatically published on the website at ProteomicsML.org and are freely accessible for other researchers.

For many LC-IM-MS properties, such as retention time and fragmentation intensity, well-performing ML models have already been published. We aim to provide suitable datasets and tutorials to easily reproduce these results in an educational fashion. All datasets on the platform are organized by data type, and should ideally be provided in a simple format that is suitable for direct import into ML toolkits. Each data type can contain one or more datasets for different purposes, and each dataset should be sufficiently annotated with metadata, e.g. its origin, how it was processed, and relevant citations.
Along with well-annotated datasets, the platform provides users with in-depth tutorials on how to download, import, handle, and train various ML models. Many of the LC-IM-MS data types require certain, sometimes complicated, preprocessing steps in order to be fully compatible with ML frameworks. For this reason, we believe it to be crucial to provide guidelines on these processes to ultimately lower the entry barriers for new users to the field. Tutorials on ProteomicsML can be attribute- or dataset-specific, allowing new tutorial submissions to focus on the direct interactions with specific ML models or methodologies, or to focus on a certain aspect of data preprocessing.

Often when new modeling approaches are published, they are accompanied by datasets with novel pre- and post-processing steps. With ProteomicsML, the new data and approach can be uploaded to the site along with a unified metadata entry and an accompanying tutorial that improves reproducibility and facilitates benchmarking by the community.


## Data types

The original raw data for proteomics datasets currently included in ProteomicsML.org have already been made publicly available through ProteomeXchange [@Deutsch2020-og], mostly via the PRIDE Archive [@Perez-Riverol2022-ak]. Instead, the data hosted at ProteomicsML are provided in an ML-ready format, with links to original metadata and raw files for full provenance. Even though the datasets at ProteomicsML do not contain raw files, we do aim to provide users with extensive tutorials on how to process raw data into ML-ready formats.
ProteomicsML currently contains datasets and tutorials for fragmentation intensity, ion mobility, retention time, and protein detectability. More data types can easily be added in the future, as the platform evolves along with the field.

(1) Retention time. Due to retention time playing a major role in modern peptide identification workflows, it is one of the most explored properties in predictive proteomics. This is why we have provided multiple retention time datasets from various sources. We have combined several previously released ML-ready datasets - such as the Sharma et al. dataset from Kaggle and the DLOmix dataset - with a newly compiled multi-tiered dataset from the ProteomeTools synthetic peptide library [@Zolg2017-ys]. From the latter, we have generated datasets of three sizes: 100,000 data points (small), well suited for newcomers; (ii) 250,000 data points (medium), and (iii) 1 million data points (large), well suited for larger-scale ML training or benchmarking. As amino acid modifications can complicate the application of ML in proteomics, these three tiers do not contain any modified peptides. Nevertheless, to train models for more real-life applications, we have also included an additional dataset tier containing 150,000 oxidized peptides, as well as a mixed dataset containing 150,000 oxidized and 150,000 unmodified peptides. These datasets require minimal data preparation, although we still provide two distinct tutorials on methods to incorporate these datasets into deep learning (DL) based  models. In addition to preprocessed data, we also provide a detailed tutorial that combines and aligns retention times between runs from MaxQuant evidence files [@tyanova2016-ma]. The output of this tutorial is a fully ML-ready file for retention time prediction.

(2) Fragmentation intensity. While it is easy to calculate the m/z values of theoretical peptide spectra, fragment ion peak intensities follow complex patterns that can be hard to predict. Nevertheless, these intensities can play a key role in accurate peptide identification [@C_Silva2019-yy]. For this reason, fragment ion intensity prediction is likely the second most explored topic, and which is why we choose to implement comprehensive datasets and tutorials for this data type. Since there are many attributes of peptides that affect their fragmentation patterns, the pre-processing steps of fragmentation data are more complex, and can be substantially different from lab to lab. For this reason, we have composed two separate tutorials, one that mimics the Prosit data processing approach on the ProteomeTools datasets, and one that mimics the MS2PIP data process on a consensus human HCD dataset [@Gabriels2019-gx] For datasets in this category it is difficult to provide a simple format with unified columns, as the handling and pre-processing steps differ significantly from model to model. Currently, there is one tutorial available on ProteomicsML describing the data processing pipeline from raw file to Prosit-style annotation, and we believe that with future additions we can provide users with tutorials for additional processing approaches.

(3) Ion mobility. Ion mobility is a technique to separate ionized analytes based on their size, shape, and physio-chemical properties [@Dodds2019-oi]. Initially the techniques for ion mobility propelled the ions with an electric field through a cell with inert gas where the ions collide with the inert gas without fragmentation. Separation is then achieved by the ions traveling faster or slower in the electric field (i.e., based on their charge) through the collisions with the gas (i.e., based on shape and size). Traveling wave ion mobility (TWIMS) works on the same principle but pushes the ions forward through the ion mobility cell with a wave of electric field [@Shvartsburg2008-ir]. Trapped ion mobility (TIMS) reverses this operation by trapping the ions in an electric field and forcing them forward by collision with the gas [@Michelmann2015-nu]. From any of the different ion mobility techniques one can derive the collisional cross-section (CCS) in Ångströms squared with the use of calibration analytes that have a known CCS. Historically most methods were based on molecular dynamics models that calculate the CCS from first principles in physics [@Larriba-Andaluz2020-kc]. Lately the field has published multiple ML and DL approaches for both peptide and metabolite CCS prediction [@Zhou2017-ee;@Broeckling2021-ks;@Meier2021-ig]. The tutorials made available in ProteomicsML use both TIMS and TWIMS data, where the large TIMS data set is from Meier et al [@Meier2021-ig]. (718,917 data points) and the TWIMS data is from Puyvelde et al [@Van_Puyvelde2022-nv]. (6268 data points). The tutorial is a walkthrough that trains linear models to more complex non-linear models (e.g., DL based networks) showing advantages and disadvantages of the learning algorithms for CCS prediction.

(4) Protein detectability. Modern proteomic methods and instrumentation are now routinely detecting and quantifying the majority of proteins thought to be encoded by the genome of a species [@Hebert2014-tc]. Yet even after gathering enormous amounts of data, there is always a subset of proteins that remains refractory to detection. For example, even through tremendous effort focused on the human proteome, the fraction of unobserved proteins has been pushed just below 10% [@Adhikari2020-vu;@Omenn2021-qc]. It remains unclear why certain proteins remain undetected, though machine learning has been applied to explore which properties most strongly influence detectability [as reviewed within @Dincer2022-re]. One can compute a set of properties for a proteome and then train a model using those properties based on real world observations of the proteins that are detected and the proteins that aren’t detected. The model can be trained to learn which properties separate the detected from the undetected. Such a model has further utility to highlight proteins that have properties that should make them belong to the detected group, but yet are not, as well as proteins that should belong to the undetected group, and yet they are detected. To facilitate this we have included a dataset that is based on an extensive study of a proteome: [Arabidopsis PeptideAtlas](http://www.peptideatlas.org/builds/arabidopsis/). This dataset is based on the 2021 build, which has 52 datasets reprocessed to yield 40 million peptide-spectrum matches and good coverage of the Arabidopsis thaliana proteome. Proteins in the dataset are categorized as either “canonical”, the strongest evidence of detection, or “not observed” if known peptides are not identified. Along with these class labels, the dataset contains various protein properties such as molecular weight, hydrophobicity, and isoelectric point that could be crucial for classification purposes. The dataset has an accompanying tutorial that illustrates how to analyze the data with a multilayer perceptron model to classify the observability of peptides.

Overall, these initial dataset submissions and tutorials leave room for a range of future expansion, until the community resource contains datasets for all properties previously and currently being explored in the field of proteomics. It is also open for user submissions, allowing researchers to upload their data in a standardized fashion for more reproducible science, along with in-depth tutorials on their data handling and ML methodologies. Our hope is that this will shape the future of predictive proteomics, in favor of being more introducible, standardized and reproducible.

Additionally, we have compiled a list of proteomics publications that utilize ML, along with a list of ProteomeXchange datasets used by each of the publications ([Supplementary Table 1](https://docs.google.com/spreadsheets/d/1oo_f_8Y15HfEXTxPS8nyl1ffw31cpO9HmC8pfUCgat4/edit?usp=sharing)). Each of these ProteomeXchange datasets have been given a set of tags to indicate the nature of the usage in the publications (e.g., benchmarking, retention time, deep learning, etc.) (see [github.com/PRIDE-Utilities/pride-ontology](https://github.com/PRIDE-Utilities/pride-ontology/blob/master/pride-annotations/projects-proteomicsML.csv)). Furthermore, these tags have also been added to the respective PRIDE entries, which allows the tags to easily be searched, and for users to compile their ideal dataset, if ProteomicsML does not already contain one.


## Conclusion

We have presented ProteomicsML.org, a comprehensive resource of datasets and tutorials for every ML practitioner in the field of MS-based proteomics. ProteomicsML contains multiple datasets on a range of LC-IM-MS peptide properties, allowing computational proteomics researchers to compare new algorithms to the state-of-the-art models, as well as providing newcomers to the field with an easier starting point without requiring immediate in-depth knowledge of the entire proteomics analysis pipeline. We believe that this resource will aid the next generation of ML practitioners, and provide a stepping stone for more open and reproducible science in the field.


## Notes

The authors declare the following competing financial interest(s): Tobias Schmidt and Siegfried Gessulat are employees of MSAID. MSAID makes machine learning-based software modules that are sold as part of Proteome Discoverer and also offers contract research. All other authors declare no competing financial interest.

Identification of certain commercial equipment, instruments, software, or materials does not imply recommendation or endorsement by the National Institute of Standards and Technology, nor does it imply that the products identified are necessarily the best available for the purpose.


## Acknowledgements
We would like to thank Wassim Gabriel and Mathias Wilhelm for consultations on the Prosit annotation pipeline.
The 2022 Lorentz Center workshop on Proteomics and Machine Learning was funded by the Dutch Research Council (NWO) with generous support from the Leiden University Medical Center, Thermo Fisher Scientific and the Journal of Proteome Research. Thanks also goes out to the staff at the Lorentz Center for helping make the hybrid workshop a success in pandemic times.


## Funding

- T.G.R acknowledges funding from the Velux Foundation [00028116]
- R.B. acknowledges funding from Vlaams Agentschap Innoveren en Ondernemen [HBC.2020.2205]
- R.G. acknowledges funding from the Research Foundation Flanders (FWO) [12B7123N]
- E.W.D. acknowledges funding from National Institutes of Health [R01GM087221, R24GM127667, U19AG023122], and by National Science Foundation grants [DBI-1933311, IOS-1922871]
- J.A.V acknowledges funding from EMBL core funding, EU H2020 [823839], and BBSRC [BB/S01781X/1, BB/V018779/1].
