{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DLOmix Prosit\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_dlomix-prosit.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BbgWdMAQ7v-"
   },
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install dlomix\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install dlomix\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BEOR5ydRMmu"
   },
   "outputs": [],
   "source": [
    "# Import and normalize/standarize data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import and normalize the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/ProteomicsML/RetentionTime/main/datasets/ProteomeTools/Small.csv')\n",
    "\n",
    "# shuffle and split dataset into internal (80%) and external (20%) datasets\n",
    "data = data.sample(frac=1)\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "data = data[:int(len(data)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ny5f5e2TZWdd",
    "outputId": "74672c0f-36b6-4f32-d151-39be4c904a28"
   },
   "outputs": [],
   "source": [
    "# Split the internal dataset into training and validation\n",
    "# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\n",
    "unique_sequences = list(set(data['Sequence']))\n",
    "# Shuffle the data to ensure unbiased data splitting\n",
    "from random import shuffle\n",
    "shuffle(unique_sequences)\n",
    "# Split sequence 80-10-10 training, validation and testing split\n",
    "train = unique_sequences[0:int(len(unique_sequences) * 0.8)]\n",
    "validation = unique_sequences[int(len(unique_sequences) * 0.8):]\n",
    "# Transfer the sequence split into data split\n",
    "train = data[data['Sequence'].isin(train)]\n",
    "validation = data[data['Sequence'].isin(validation)]\n",
    "print('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n",
    "# Here we use test as an external dataset unlike the one used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzNHLohpZX1W",
    "outputId": "14a7f118-fe96-466a-b462-a9df40e4c793"
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "if normalize:\n",
    "  # Normalize\n",
    "  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n",
    "  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\n",
    "else:\n",
    "  # Standardize\n",
    "  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n",
    "  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n",
    "  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n",
    "  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDPvGCs8aHBs"
   },
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuC056d-fkPi",
    "outputId": "f5c07dca-3d73-48a1-9927-214d7fadf245"
   },
   "outputs": [],
   "source": [
    "# Setup data \n",
    "from dlomix.data import RetentionTimeDataset\n",
    "train_input = RetentionTimeDataset(data_source=tuple([np.array(train['Sequence']), np.array(train['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "val_input = RetentionTimeDataset(data_source=tuple([np.array(validation['Sequence']), np.array(validation['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "test_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['Sequence']), np.array(test_data['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "# Setup PROSIT model from DLOmix\n",
    "from dlomix.models.prosit import PrositRetentionTimePredictor\n",
    "model = PrositRetentionTimePredictor(seq_length=sequence_length)\n",
    "model.build((None, sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nHIFG_XXRp4",
    "outputId": "0056064f-42e0-40f8-a8c0-57d213da10c2"
   },
   "outputs": [],
   "source": [
    "from dlomix.eval.rt_eval import TimeDeltaMetric\n",
    "import tensorflow as tf\n",
    "# Compiling the keras model with loss function, metrics and optimizer\n",
    "model.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "# Train the model\n",
    "history = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_5_nipgkSZ0",
    "outputId": "b8d2e25f-b160-41f1-a80b-9383fe38732a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the training history \n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.plot(range(epochs), history.history['val_loss'], '--', color='r', label='Validation loss')\n",
    "plt.title(f'Training and validation loss across epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nb1ECgqPZm1R",
    "outputId": "57e1a22f-7e56-4f4d-e918-9c566cd79fd0"
   },
   "outputs": [],
   "source": [
    "# Initially we trained on just one gradient, to transfer this model to external datasets, \n",
    "# we refine the model by using the model we just trained as a pre-trained model, and then further train it with the test/external dataset\n",
    "history = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\n",
    "# The model can now be used for other datasets with the same gradient set-up\n",
    "# We then plot the history of this model, and see the initial performance is much better, \n",
    "# as the model already has some gradient agnostic knowledge, and it simply has to learn the new gradients\n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.title(f'Training and validation loss of the refined model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c339fba8800f289223ed8cfd6e3b81401a206d437a496d69810d81cfdc690c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
