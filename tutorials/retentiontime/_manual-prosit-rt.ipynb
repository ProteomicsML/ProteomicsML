{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Manual embedding of Bi-LSTM model on ProteomeTools data\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Tobias Greisager Rehfeldt\n",
    "\n",
    "  orcid: 0000-0002-1190-9485\n",
    "\n",
    "  affiliations:\n",
    "    - University of Southern Denmark, Odense\n",
    "    - Department of Natural Science, Institute for Mathematics and Computer Science\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_manual-prosit-rt.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%capture \n",
    "!pip install pandas sklearn tensorflow dlomix numpy matplotlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and normalize/standarize data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import and normalize the data\n",
    "data = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/small.zip?raw=true', compression='zip')\n",
    "\n",
    "# shuffle and split dataset into internal (80%) and external (20%) datasets\n",
    "data = data.sample(frac=1)\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "data = data[:int(len(data)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the internal dataset into training and validation\n",
    "# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\n",
    "unique_sequences = list(set(data['sequence']))\n",
    "# Shuffle the data to ensure unbiased data splitting\n",
    "from random import shuffle\n",
    "shuffle(unique_sequences)\n",
    "# Split sequence 80-10-10 training, validation and testing split\n",
    "train = unique_sequences[0:int(len(unique_sequences) * 0.8)]\n",
    "validation = unique_sequences[int(len(unique_sequences) * 0.8):]\n",
    "# Transfer the sequence split into data split\n",
    "train = data[data['sequence'].isin(train)]\n",
    "validation = data[data['sequence'].isin(validation)]\n",
    "print('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n",
    "# Here we use test as an external dataset unlike the one used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True\n",
    "if normalize:\n",
    "  # Normalize\n",
    "  train_val_min, train_val_max = min(train['retention time'].min(), validation['retention time'].min()), max(train['retention time'].max(), validation['retention time'].max())\n",
    "  train['retention time'] = list((train['retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  validation['retention time'] = list((validation['retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  test_data['retention time'] = list((test_data['retention time'] - test_data['retention time'].min()) / (test_data['retention time'].max() - test_data['retention time'].min()))\n",
    "else:\n",
    "  # Standardize\n",
    "  train_val_mean, train_val_std = np.mean(list(train['retention time']) + list(validation['retention time'])), np.std(list(train['retention time']) + list(validation['retention time']))\n",
    "  train['retention time'] = (train['retention time'] - train_val_mean) / train_val_std\n",
    "  validation['retention time'] = (validation['retention time'] - train_val_mean) / train_val_std\n",
    "  test_data['retention time'] = (test_data['retention time'] - np.mean(test_data['retention time'])) / np.std(test_data['retention time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual sequence embedding\n",
    "# Remove sequences longer than our maximum sequence length\n",
    "train = train[train['sequence'].str.len()<=sequence_length]\n",
    "validation = validation[validation['sequence'].str.len()<=sequence_length]\n",
    "test_data = test_data[test_data['sequence'].str.len()<=sequence_length]\n",
    "\n",
    "# Create an alphabet to convert from string to numeric\n",
    "AA_alphabet = {\"A\": 1, \"C\": 2, \"D\": 3, \"E\": 4, \"F\": 5, \"G\": 6, \"H\": 7, \"I\": 8, \"K\": 9, \"L\": 10, \"M\": 11, \"N\": 12, \"P\": 13, \"Q\": 14, \"R\": 15, \"S\": 16, \"T\": 17, \"V\": 18, \"W\": 19, \"Y\": 20}\n",
    "# Convert sequences from string to numberic\n",
    "embedded_sequences_train = [[AA_alphabet[g] for g in f] for f in train['sequence']]\n",
    "embedded_sequences_validation = [[AA_alphabet[g] for g in f] for f in validation['sequence']]\n",
    "embedded_sequences_test = [[AA_alphabet[g] for g in f] for f in test_data['sequence']]\n",
    "\n",
    "# Make sure every sequence is the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "embedded_sequences_train = pad_sequences(sequences=embedded_sequences_train, maxlen=sequence_length)\n",
    "embedded_sequences_validation = pad_sequences(sequences=embedded_sequences_validation, maxlen=sequence_length)\n",
    "embedded_sequences_test = pad_sequences(sequences=embedded_sequences_test, maxlen=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed layers and tensorflow model requirements\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Concatenate, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(sequence_length,), name='Input')\n",
    "# Embed the sequnces in a 20 x 8 matrix\n",
    "input_embedding = Embedding(input_dim=len(AA_alphabet)+2, output_dim=8, name='Sequence_Embedding')(inputs)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True), name='Bi_LSTM_1')(input_embedding)\n",
    "x = Dropout(0.25, name='LSTM_Dropout')(x)\n",
    "x = Bidirectional(LSTM(32), name='Bi_LSTM_2')(x)\n",
    "output = Dense(1, activation=\"linear\", name='Output')(x)\n",
    "model = Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Compiling the keras model with loss function, metrics and optimizer\n",
    "model.compile(loss='mse', metrics=['mae'], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "# Train the model\n",
    "history = model.fit(x=embedded_sequences_train, y=train['retention time'], epochs=epochs, \n",
    "                    batch_size=batch_size, validation_data=(embedded_sequences_validation, validation['retention time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the training history \n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.plot(range(epochs), history.history['val_loss'], '--', color='r', label='Validation loss')\n",
    "plt.title(f'Training and validation loss across epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially we trained on just one gradient, to transfer this model to external datasets, \n",
    "# we refine the model by using the model we just trained as a pre-trained model, and then further train it with the test/external dataset\n",
    "history = model.fit(x=embedded_sequences_test, y=test_data['retention time'], epochs=epochs, batch_size=batch_size)\n",
    "# The model can now be used for other datasets with the same gradient set-up\n",
    "# We then plot the history of this model, and see the initial performance is much better, \n",
    "# as the model already has some gradient agnostic knowledge, and it simply has to learn the new gradients\n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.title(f'Training and validation loss of the refined model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
