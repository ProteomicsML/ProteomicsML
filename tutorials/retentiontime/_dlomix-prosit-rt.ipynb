{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DLOmix embedding of Prosit model on ProteomeTools data\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Tobias Greisager Rehfeldt\n",
    "\n",
    "  orcid: 0000-0002-1190-9485\n",
    "\n",
    "  affiliations:\n",
    "    - University of Southern Denmark, Odense\n",
    "    - Department of Natural Science, Institute for Mathematics and Computer Science\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_dlomix-prosit-rt.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%capture\n",
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install tensorflow\n",
    "!pip install dlomix\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install dlomix\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and normalize/standarize data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import and normalize the data\n",
    "data = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/Small.csv.gz?raw=true', compression='gzip')\n",
    "\n",
    "# shuffle and split dataset into internal (80%) and external (20%) datasets\n",
    "data = data.sample(frac=1)\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "data = data[:int(len(data)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the internal dataset into training and validation\n",
    "# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\n",
    "unique_sequences = list(set(data['Sequence']))\n",
    "# Shuffle the data to ensure unbiased data splitting\n",
    "from random import shuffle\n",
    "shuffle(unique_sequences)\n",
    "# Split sequence 80-10-10 training, validation and testing split\n",
    "train = unique_sequences[0:int(len(unique_sequences) * 0.8)]\n",
    "validation = unique_sequences[int(len(unique_sequences) * 0.8):]\n",
    "# Transfer the sequence split into data split\n",
    "train = data[data['Sequence'].isin(train)]\n",
    "validation = data[data['Sequence'].isin(validation)]\n",
    "print('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n",
    "# Here we use test as an external dataset unlike the one used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True\n",
    "if normalize:\n",
    "  # Normalize\n",
    "  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n",
    "  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\n",
    "else:\n",
    "  # Standardize\n",
    "  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n",
    "  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n",
    "  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n",
    "  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data \n",
    "from dlomix.data import RetentionTimeDataset\n",
    "train_input = RetentionTimeDataset(data_source=tuple([np.array(train['Sequence']), np.array(train['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "val_input = RetentionTimeDataset(data_source=tuple([np.array(validation['Sequence']), np.array(validation['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "test_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['Sequence']), np.array(test_data['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "# Setup PROSIT model from DLOmix\n",
    "from dlomix.models.prosit import PrositRetentionTimePredictor\n",
    "model = PrositRetentionTimePredictor(seq_length=sequence_length)\n",
    "model.build((None, sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.eval.rt_eval import TimeDeltaMetric\n",
    "import tensorflow as tf\n",
    "# Compiling the keras model with loss function, metrics and optimizer\n",
    "model.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "# Train the model\n",
    "history = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.reports import RetentionTimeReport\n",
    "report = RetentionTimeReport(output_path=\"./output\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"timedelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.concatenate([y for x, y in val_input], axis=0)\n",
    "y_pred = model.predict(validation['Sequence'][:len(y_real)])\n",
    "report.plot_residuals(y_real, y_pred, xrange=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.title(f'Training and validation loss of the refined model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
