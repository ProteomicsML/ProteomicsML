{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DLOmix embedding of Prosit model on ProteomeTools data\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Tobias Greisager Rehfeldt\n",
    "\n",
    "  orcid: 0000-0002-1190-9485\n",
    "\n",
    "  affiliations:\n",
    "    - University of Southern Denmark, Odense\n",
    "    - Department of Natural Science, Institute for Mathematics and Computer Science\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_dlomix-prosit-rt.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.3.5 sklearn==0.0.post1 tensorflow==2.9.2 dlomix==0.0.3 numpy==1.21.6 matplotlib==3.2.2 requests==2.23.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and normalize/standarize data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import and normalize the data\n",
    "data = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/small.zip?raw=true', compression='zip')\n",
    "\n",
    "# shuffle and split dataset into internal (80%) and external (20%) datasets\n",
    "data = data.sample(frac=1)\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "data = data[:int(len(data)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the internal dataset into training and validation\n",
    "# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\n",
    "unique_sequences = list(set(data['sequence']))\n",
    "# Shuffle the data to ensure unbiased data splitting\n",
    "from random import shuffle\n",
    "shuffle(unique_sequences)\n",
    "# Split sequence 80-10-10 training, validation and testing split\n",
    "train = unique_sequences[0:int(len(unique_sequences) * 0.8)]\n",
    "validation = unique_sequences[int(len(unique_sequences) * 0.8):]\n",
    "# Transfer the sequence split into data split\n",
    "train = data[data['sequence'].isin(train)]\n",
    "validation = data[data['sequence'].isin(validation)]\n",
    "print('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n",
    "# Here we use test as an external dataset unlike the one used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True\n",
    "if normalize:\n",
    "  # Normalize\n",
    "  train_val_min, train_val_max = min(train['retention time'].min(), validation['retention time'].min()), max(train['retention time'].max(), validation['retention time'].max())\n",
    "  train['retention time'] = list((train['retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  validation['retention time'] = list((validation['retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  test_data['retention time'] = list((test_data['retention time'] - test_data['retention time'].min()) / (test_data['retention time'].max() - test_data['retention time'].min()))\n",
    "else:\n",
    "  # Standardize\n",
    "  train_val_mean, train_val_std = np.mean(list(train['retention time']) + list(validation['retention time'])), np.std(list(train['retention time']) + list(validation['retention time']))\n",
    "  train['retention time'] = (train['retention time'] - train_val_mean) / train_val_std\n",
    "  validation['retention time'] = (validation['retention time'] - train_val_mean) / train_val_std\n",
    "  test_data['retention time'] = (test_data['retention time'] - np.mean(test_data['retention time'])) / np.std(test_data['retention time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data \n",
    "from dlomix.data import RetentionTimeDataset\n",
    "train_input = RetentionTimeDataset(data_source=tuple([np.array(train['sequence']), np.array(train['retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "val_input = RetentionTimeDataset(data_source=tuple([np.array(validation['sequence']), np.array(validation['retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "test_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['sequence']), np.array(test_data['retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "# Setup PROSIT model from DLOmix\n",
    "from dlomix.models.prosit import PrositRetentionTimePredictor\n",
    "model = PrositRetentionTimePredictor(seq_length=sequence_length)\n",
    "model.build((None, sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.eval.rt_eval import TimeDeltaMetric\n",
    "import tensorflow as tf\n",
    "# Compiling the keras model with loss function, metrics and optimizer\n",
    "model.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "# Train the model\n",
    "history = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlomix.reports import RetentionTimeReport\n",
    "report = RetentionTimeReport(output_path=\"./output\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"timedelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_real = np.concatenate([y for x, y in val_input], axis=0)\n",
    "y_pred = model.predict(validation['sequence'][:len(y_real)])\n",
    "report.plot_residuals(y_real, y_pred, xrange=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.title(f'Training and validation loss of the refined model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
