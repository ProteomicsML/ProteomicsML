{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMYIdDFj6Mp8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "title: \"DLOmix embedding of Prosit model on ProteomeTools data\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Tobias Greisager Rehfeldt\n",
    "\n",
    "  orcid: 0000-0002-1190-9485\n",
    "\n",
    "  affiliations:\n",
    "    - University of Southern Denmark, Odense\n",
    "    - Department of Natural Science, Institute for Mathematics and Computer Science\n",
    "\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_dlomix-prosit.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BbgWdMAQ7v-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas sklearn tensorflow dlomix numpy matplotlib dlomix requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BEOR5ydRMmu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import and normalize/standarize data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Import and normalize the data\n",
    "data = pd.read_csv('https://github.com/ProteomicsML/ProteomicsML/blob/main/datasets/retentiontime/ProteomeTools/Small.csv.gz?raw=true', compression='gzip')\n",
    "\n",
    "# shuffle and split dataset into internal (80%) and external (20%) datasets\n",
    "data = data.sample(frac=1)\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "data = data[:int(len(data)*0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ny5f5e2TZWdd",
    "outputId": "0f578cca-f06d-4057-8caa-3cebc7fa5511",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the internal dataset into training and validation\n",
    "# We have to split the data based on Sequences, to make sure we dont have cross-over sequences in the training and validation splits.\n",
    "unique_sequences = list(set(data['Sequence']))\n",
    "# Shuffle the data to ensure unbiased data splitting\n",
    "from random import shuffle\n",
    "shuffle(unique_sequences)\n",
    "# Split sequence 80-10-10 training, validation and testing split\n",
    "train = unique_sequences[0:int(len(unique_sequences) * 0.8)]\n",
    "validation = unique_sequences[int(len(unique_sequences) * 0.8):]\n",
    "# Transfer the sequence split into data split\n",
    "train = data[data['Sequence'].isin(train)]\n",
    "validation = data[data['Sequence'].isin(validation)]\n",
    "print('Training data points:', len(train),'  Validation data points:',  len(validation),'  Testing data points:',  len(test_data))\n",
    "# Here we use test as an external dataset unlike the one used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzNHLohpZX1W",
    "outputId": "73d6c9b2-04c2-4338-a23e-78df074a73fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "if normalize:\n",
    "  # Normalize\n",
    "  train_val_min, train_val_max = min(train['Retention time'].min(), validation['Retention time'].min()), max(train['Retention time'].max(), validation['Retention time'].max())\n",
    "  train['Retention time'] = list((train['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  validation['Retention time'] = list((validation['Retention time'] - train_val_min) / (train_val_max - train_val_min))\n",
    "  test_data['Retention time'] = list((test_data['Retention time'] - test_data['Retention time'].min()) / (test_data['Retention time'].max() - test_data['Retention time'].min()))\n",
    "else:\n",
    "  # Standardize\n",
    "  train_val_mean, train_val_std = np.mean(list(train['Retention time']) + list(validation['Retention time'])), np.std(list(train['Retention time']) + list(validation['Retention time']))\n",
    "  train['Retention time'] = (train['Retention time'] - train_val_mean) / train_val_std\n",
    "  validation['Retention time'] = (validation['Retention time'] - train_val_mean) / train_val_std\n",
    "  test_data['Retention time'] = (test_data['Retention time'] - np.mean(test_data['Retention time'])) / np.std(test_data['Retention time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDPvGCs8aHBs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "sequence_length = 30\n",
    "batch_size = 64\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuC056d-fkPi",
    "outputId": "4ed559e8-c4a7-463a-faaf-b395973f6a5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup data \n",
    "from dlomix.data import RetentionTimeDataset\n",
    "train_input = RetentionTimeDataset(data_source=tuple([np.array(train['Sequence']), np.array(train['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "val_input = RetentionTimeDataset(data_source=tuple([np.array(validation['Sequence']), np.array(validation['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "test_input = RetentionTimeDataset(data_source=tuple([np.array(test_data['Sequence']), np.array(test_data['Retention time'])]), \n",
    "                                        seq_length=sequence_length, batch_size=batch_size, test=False).train_data\n",
    "\n",
    "# Setup PROSIT model from DLOmix\n",
    "from dlomix.models.prosit import PrositRetentionTimePredictor\n",
    "model = PrositRetentionTimePredictor(seq_length=sequence_length)\n",
    "model.build((None, sequence_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nHIFG_XXRp4",
    "outputId": "eddc884d-c858-44b8-eac5-609a8e0e6392",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dlomix.eval.rt_eval import TimeDeltaMetric\n",
    "import tensorflow as tf\n",
    "# Compiling the keras model with loss function, metrics and optimizer\n",
    "model.compile(loss='mse', metrics=['mae', TimeDeltaMetric()], optimizer=tf.keras.optimizers.Adam(learning_rate=0.005))\n",
    "# Train the model\n",
    "history = model.fit(x=train_input, epochs=epochs, batch_size=batch_size, validation_data=val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgyVf9MU7md5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dlomix.reports import RetentionTimeReport\n",
    "report = RetentionTimeReport(output_path=\"./output\", history=history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "BEdF4Df6BPhc",
    "outputId": "8f5ed1ec-04e6-443b-dfb7-50eee15e304d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "pxWmKDUjBTxa",
    "outputId": "043529bc-b712-469b-b752-fa6bc8a1e8d7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "report.plot_keras_metric(\"timedelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ODjYhR-LFm0M",
    "outputId": "baa23d90-0d27-4a6d-aeb2-5022ce6719b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_real = np.concatenate([y for x, y in val_input], axis=0)\n",
    "y_pred = model.predict(validation['Sequence'][:len(y_real)])\n",
    "report.plot_residuals(y_real, y_pred, xrange=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "Nb1ECgqPZm1R",
    "outputId": "09cf789f-db60-49c0-c40b-fe60b69f4db3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=test_input, epochs=epochs, batch_size=batch_size)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.title(f'Training and validation loss of the refined model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
