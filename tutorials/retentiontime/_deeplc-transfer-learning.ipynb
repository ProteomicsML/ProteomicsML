{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6430f98f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Transfer learning with DeepLC\"\n",
    "date: last-modified\n",
    "author:\n",
    "- name: Robbin Bouwmeester\n",
    "  orcid: 0000-0001-6807-7029\n",
    "  affiliations:\n",
    "    - VIB-UGent Center for Medical Biotechnology, VIB, Belgium\n",
    "    - Department of Biomolecular Medicine, Ghent University, Belgium\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1708a",
   "metadata": {},
   "source": [
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/retentiontime/_deeplc-transfer-learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1683c",
   "metadata": {},
   "source": [
    "## Transfer learning with DeepLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f70d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library for transfer learning\n",
    "!pip install deeplc\n",
    "!pip install deeplcretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6502b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deeplc packages\n",
    "from deeplc import DeepLC\n",
    "from deeplcretrainer import deeplcretrainer\n",
    "\n",
    "# Default\n",
    "from collections import Counter\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# specific packages\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.eager import context\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01be20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain three models for deeplc\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/compomics/DeepLC/raw/master/deeplc/mods/full_hc_hela_hf_psms_aligned_1fd8363d9af9dcad3be7553c39396960.hdf5\",\n",
    "    \"full_hc_train_pxd001468_1fd8363d9af9dcad3be7553c39396960.hdf5\"\n",
    ")\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/compomics/DeepLC/raw/master/deeplc/mods/full_hc_hela_hf_psms_aligned_8c22d89667368f2f02ad996469ba157e.hdf5\",\n",
    "    \"full_hc_train_pxd001468_8c22d89667368f2f02ad996469ba157e.hdf5\"\n",
    ")\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/compomics/DeepLC/raw/master/deeplc/mods/full_hc_hela_hf_psms_aligned_cb975cfdd4105f97efa0b3afffe075cc.hdf5\",\n",
    "    \"full_hc_train_pxd001468_cb975cfdd4105f97efa0b3afffe075cc.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5edc6",
   "metadata": {},
   "source": [
    "In this tutorial you will learn how to apply transfer learning to DeepLC models. In previous versions of DeepLC the retention time was calibrated to the LC system that the researcher wants to apply the predictions to. This calibration was performed with either a piecewise linear function or in later versions with a GAM. However, this calibration works under the assumption the that elution order is preserved. Transfer learning has been shown to accurately model changes in chromatographic setup while requiring only a small number of peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d11490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the input csv file\n",
    "combined_df = pd.read_csv(\n",
    "    \"https://github.com/ProteomicsML/ProteomicsML/raw/combined_datasets_retention_time/datasets/retentiontime/PRIDE_MQ/PRIDE_MQ.zip?raw=true\",\n",
    "    compression=\"zip\",\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dede89b",
   "metadata": {},
   "source": [
    "We have the following columns in the downloaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21224e6",
   "metadata": {},
   "source": [
    "In this file there are multiple projects, some of these have many peptides and retention times associated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_counter = Counter(combined_df[\"project\"])\n",
    "combined_counter.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ea1e8",
   "metadata": {},
   "source": [
    "The smallest projects in the data set still have over 20 000 peptides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_counter.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a992a5",
   "metadata": {},
   "source": [
    "Lets select the project 'PXD002549' which is the project with the smallest number of peptides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ac4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df[combined_df[\"project\"] == combined_counter.most_common()[-1][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b4304f",
   "metadata": {},
   "source": [
    "From this project we take 90 % of the data for training and early stopping (5 % of this 90 %). The remaining 10 % is used for testing prediction accuracy on unseen peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876fabfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.9)\n",
    "df_test = df.loc[df.index.difference(df_train.index)]\n",
    "\n",
    "df_train.fillna(\"\",inplace=True)\n",
    "df_test.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c3628",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8e0f5",
   "metadata": {},
   "source": [
    "In this section we will use calibration to predict retention times for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af910daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# The following code is not required in most cases, but here it is used to clear variables that might cause problems\n",
    "_ = tf.Variable([1])\n",
    "\n",
    "context._context = None\n",
    "context._create_context()\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# Make sure we have no NA in the dataframes\n",
    "df_test['modifications'] = df_test['modifications'].fillna(\"\")\n",
    "df_train['modifications'] = df_train['modifications'].fillna(\"\")\n",
    "\n",
    "# Call DeepLC with the downloaded models, say that we use GAM calibration\n",
    "dlc = DeepLC(\n",
    "        path_model=[\"full_hc_train_pxd001468_1fd8363d9af9dcad3be7553c39396960.hdf5\",\n",
    "                    \"full_hc_train_pxd001468_8c22d89667368f2f02ad996469ba157e.hdf5\",\n",
    "                    \"full_hc_train_pxd001468_cb975cfdd4105f97efa0b3afffe075cc.hdf5\"],\n",
    "        batch_num=1024000,\n",
    "        pygam_calibration=True\n",
    ")\n",
    "\n",
    "# Perform calibration, make predictions and calculate metrics\n",
    "dlc.calibrate_preds(seq_df=df_train)\n",
    "preds_calib = dlc.make_preds(seq_df=df_test)\n",
    "\n",
    "mae_calib = sum(abs(df_test[\"tr\"]-preds_calib))/len(df_test[\"tr\"].index)\n",
    "perc95_calib = np.percentile(abs(df_test[\"tr\"]-preds_calib),95)*2\n",
    "cor_calib = pearsonr(df_test[\"tr\"],preds_calib)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966d5b4",
   "metadata": {},
   "source": [
    "Lets plot the results! These were fitted with a pretrained model and those predictions were calibrated with a GAM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e39053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.title(f\"MAE: {round(mae_calib,2)} 95th percentile: {round(perc95_calib,2)} R: {round(cor_calib,3)}\")\n",
    "plt.scatter(df_test[\"tr\"],preds_calib,s=1,alpha=0.5)\n",
    "plt.plot([15,115],[15,115],c=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad54ce",
   "metadata": {},
   "source": [
    "## New train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f36cfb",
   "metadata": {},
   "source": [
    "There are quite a few data points so we might actually be able to train a model from the ground up. This means it starts with random parameters in the network and will change it accordingly to the training data.\n",
    "\n",
    "**This can be slow - in order to speed this process up go to edit on the top bar, select notebook settings and select GPU as a hardware accelerator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# The following code is not required in most cases, but here it is used to clear variables that might cause problems\n",
    "_ = tf.Variable([1])\n",
    "\n",
    "context._context = None\n",
    "context._create_context()\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# For training new models we need to use a file, so write the train df to a file\n",
    "df_train.to_csv(\"train.csv\",index=False)\n",
    "df_train_file = \"train.csv\"\n",
    "\n",
    "# Here we will train a new model so we keep the 'mods_transfer_learning' empty\n",
    "models_subtr = deeplcretrainer.retrain(\n",
    "    [df_train_file],\n",
    "    mods_transfer_learning=[],\n",
    "    freeze_layers=False,\n",
    "    n_epochs=100\n",
    ")\n",
    "\n",
    "# The following code is not required in most cases, but here it is used to clear variables that might cause problems\n",
    "_ = tf.Variable([1])\n",
    "\n",
    "context._context = None\n",
    "context._create_context()\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# Make a DeepLC object with the models trained previously\n",
    "dlc = DeepLC(\n",
    "        path_model=models_subtr,\n",
    "        batch_num=1024000,\n",
    "        pygam_calibration=False\n",
    ")\n",
    "\n",
    "# Perform calibration, make predictions and calculate metrics\n",
    "dlc.calibrate_preds(seq_df=df_train)\n",
    "preds_newtrain = dlc.make_preds(seq_df=df_test)\n",
    "\n",
    "mae_newtrain = sum(abs(df_test[\"tr\"]-preds_newtrain))/len(df_test[\"tr\"].index)\n",
    "perc95_newtrain = np.percentile(abs(df_test[\"tr\"]-preds_newtrain),95)*2\n",
    "cor_newtrain = pearsonr(df_test[\"tr\"],preds_newtrain)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb629cf",
   "metadata": {},
   "source": [
    "Lets plot the results of the newly trained model. As you will see the results are very comparable to calibration. In most cases with less training data the calibration strategy will outperform a newly trained model. What strategy works best, calibration or newly trained, depends highly on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.title(f\"MAE: {round(mae_newtrain,2)} 95th percentile: {round(perc95_newtrain,2)} R: {round(cor_newtrain,3)}\")\n",
    "plt.scatter(df_test[\"tr\"],preds_newtrain,s=1,alpha=0.5)\n",
    "plt.plot([15,115],[15,115],c=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeae7a90",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31208d2",
   "metadata": {},
   "source": [
    "Transfer learning is a proven strategy when making predictions for a smaller data set that does not have exactly the same objective. Instead of starting with random starting parameters the training starts with previously trained parameters. This means that it can use information from previously trained on data to obtain a better solution for the current data set.\n",
    "\n",
    "**This can be slow - in order to speed this process up go to edit on the top bar, select notebook settings and select GPU as a hardware accelerator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af56bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# The following code is not required in most cases, but here it is used to clear variables that might cause problems\n",
    "_ = tf.Variable([1])\n",
    "\n",
    "context._context = None\n",
    "context._create_context()\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# For training new models we need to use a file, so write the train df to a file\n",
    "df_train.to_csv(\"train.csv\",index=False)\n",
    "df_train_file = \"train.csv\"\n",
    "\n",
    "# Here we will apply transfer learning we specify previously trained models in the 'mods_transfer_learning'\n",
    "models = deeplcretrainer.retrain(\n",
    "    [df_train_file],\n",
    "    mods_transfer_learning=[\n",
    "        \"full_hc_train_pxd001468_1fd8363d9af9dcad3be7553c39396960.hdf5\",\n",
    "        \"full_hc_train_pxd001468_8c22d89667368f2f02ad996469ba157e.hdf5\",\n",
    "        \"full_hc_train_pxd001468_cb975cfdd4105f97efa0b3afffe075cc.hdf5\"\n",
    "    ],\n",
    "    freeze_layers=True,\n",
    "    n_epochs=10,\n",
    "    freeze_after_concat=1\n",
    ");\n",
    "\n",
    "# The following code is not required in most cases, but here it is used to clear variables that might cause problems\n",
    "_ = tf.Variable([1])\n",
    "\n",
    "context._context = None\n",
    "context._create_context()\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "\n",
    "# Make a DeepLC object with the models trained previously\n",
    "dlc = DeepLC(\n",
    "        path_model=models,\n",
    "        batch_num=1024000,\n",
    "        pygam_calibration=False\n",
    ")\n",
    "\n",
    "# Perform calibration, make predictions and calculate metrics\n",
    "dlc.calibrate_preds(seq_df=df_train)\n",
    "preds_transflearn = dlc.make_preds(seq_df=df_test)\n",
    "\n",
    "mae_transflearn = sum(abs(df_test[\"tr\"]-preds_transflearn))/len(df_test[\"tr\"].index)\n",
    "perc95_transflearn = np.percentile(abs(df_test[\"tr\"]-preds_transflearn),95)*2\n",
    "cor_transflearn = pearsonr(df_test[\"tr\"],preds_transflearn)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f65083",
   "metadata": {},
   "source": [
    "Lets have a look at the transfer learning results. As you can see in the following plot the performance is substantially higher compared to calibration or training a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca05027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.title(f\"MAE: {round(mae_transflearn,2)} 95th percentile: {round(perc95_transflearn,2)} R: {round(cor_transflearn,3)}\")\n",
    "plt.scatter(df_test[\"tr\"],preds_transflearn,s=1,alpha=0.5)\n",
    "plt.plot([15,115],[15,115],c=\"grey\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
