{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NIST (part 2): Traditional ML: Gradient boosting\"\n",
    "\n",
    "date: 2022-10-05\n",
    "date-modified: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Ralf Gabriels\n",
    "\n",
    "  orcid: 0000-0002-1679-1711\n",
    "\n",
    "  affiliations:\n",
    "    - VIB-UGent Center for Medical Biotechnology, VIB, Belgium\n",
    "    - Department of Biomolecular Medicine, Ghent University, Belgium\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/fragmentation/_nist-2-traditional-ml-gradient-boosting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This is the second part in a three-part tutorial. We recommend you to to start with the\n",
    "first section, where the NIST spectral library is parsed and prepared for use in the\n",
    "second and third parts.\n",
    "\n",
    "1. [Preparing a spectral library for ML][part1]\n",
    "2. [Traditional ML: Gradient boosting][part2]\n",
    "3. [Deep learning: BiLSTM][part3]\n",
    "\n",
    "[part1]: https://www.proteomicsml.org/tutorials/fragmentation/nist-1-parsing-spectral-library.html\n",
    "[part2]: https://www.proteomicsml.org/tutorials/fragmentation/nist-2-traditional-ml-gradient-boosting.html\n",
    "[part3]: https://www.proteomicsml.org/tutorials/fragmentation/nist-3-deep-learning-lstm.html\n",
    "\n",
    "\n",
    "In this tutorial, you will learn how to build a fragmentation intensity predictor\n",
    "similar to MS²PIP v3 [@Gabriels2019] with traditional machine learning (ML) feature\n",
    "engineering and Gradient Boosting [@Friedman2002]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required python packages\n",
    "! pip install rich~=12.5 numpy~=1.21 pandas~=1.3 matplotlib~=3.5 seaborn~=0.11 scikit-learn~=1.0 pyarrow~=15.0 hyperopt~=0.2 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "We will use the spectral library that was already parsed in part 1 of this\n",
    "tutorial series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_val_spectra = pd.read_feather(\"http://ftp.pride.ebi.ac.uk/pub/databases/pride/resources/proteomicsml/fragmentation/nist-humanhcd20160503-parsed-trainval.feather\")\n",
    "test_spectra = pd.read_feather(\"http://ftp.pride.ebi.ac.uk/pub/databases/pride/resources/proteomicsml/fragmentation/nist-humanhcd20160503-parsed-test.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Feature engineering\n",
    "\n",
    "In traditional ML, the input features for the algorithm usually require some\n",
    "engineering. For fragmentation intensity prediction this is not different. Following\n",
    "the MS²PIP methods, we will calculate the distributions of several amino acid properties\n",
    "across the peptide and fragment ion sequences.\n",
    "\n",
    "Using the distribution of these properties instead of the actual properties per amino\n",
    "acid allows MS²PIP to get a fixed length feature matrix for input peptides with varying\n",
    "lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "properties = np.array([\n",
    "    [37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104],  # basicity\n",
    "    [68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55],  # helicity\n",
    "    [51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70],  # hydrophobicity\n",
    "    [32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28],  # pI\n",
    "])\n",
    "\n",
    "pd.DataFrame(properties, columns=amino_acids, index=[\"basicity\", \"helicity\", \"hydrophobicity\", \"pI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_peptide(sequence, charge):\n",
    "    # 4 properties * 5 quantiles * 3 ion types + 4 properties * 4 site + 2 global\n",
    "    n_features = 78\n",
    "    quantiles = [0, 0.25, 0.5, 0.75, 1]\n",
    "    n_ions = len(sequence) - 1\n",
    "\n",
    "    # Encode amino acids as integers to index amino acid properties for peptide sequence\n",
    "    aa_indices = {aa: i for i, aa in  enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "    aa_to_index = np.vectorize(lambda aa: aa_indices[aa])\n",
    "    peptide_indexed = aa_to_index(np.array(list(sequence)))\n",
    "    peptide_properties = properties[:, peptide_indexed]\n",
    "\n",
    "    # Empty peptide_features array\n",
    "    peptide_features = np.full((n_ions, n_features), np.nan)\n",
    "\n",
    "    for b_ion_number in range(1, n_ions + 1):\n",
    "        # Calculate quantiles of features across peptide, b-ion, and y-ion\n",
    "        peptide_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties, quantiles, axis=1).transpose()\n",
    "        )\n",
    "        b_ion_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties[:,:b_ion_number], quantiles, axis=1).transpose()\n",
    "        )\n",
    "        y_ion_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties[:,b_ion_number:], quantiles, axis=1).transpose()\n",
    "        )\n",
    "\n",
    "        # Properties on specific sites: nterm, frag-1, frag+1, cterm\n",
    "        specific_site_indexes = np.array([0, b_ion_number - 1, b_ion_number, -1])\n",
    "        specific_site_properties = np.hstack(peptide_properties[:, specific_site_indexes].transpose())\n",
    "\n",
    "        # Global features: Length and charge\n",
    "        global_features = np.array([len(sequence), int(charge)])\n",
    "\n",
    "        # Assign to peptide_features array\n",
    "        peptide_features[b_ion_number - 1, 0:20] = peptide_quantiles\n",
    "        peptide_features[b_ion_number - 1, 20:40] = b_ion_quantiles\n",
    "        peptide_features[b_ion_number - 1, 40:60] = y_ion_quantiles\n",
    "        peptide_features[b_ion_number - 1, 60:76] = specific_site_properties\n",
    "        peptide_features[b_ion_number - 1, 76:78] = global_features\n",
    "\n",
    "    return peptide_features\n",
    "\n",
    "\n",
    "def generate_feature_names():\n",
    "    feature_names = []\n",
    "    for level in [\"peptide\", \"b\", \"y\"]:\n",
    "        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n",
    "            for quantile in [\"min\", \"q1\", \"q2\", \"q3\", \"max\"]:\n",
    "                feature_names.append(\"_\".join([level, aa_property, quantile]))\n",
    "    for site in [\"nterm\", \"fragmin1\", \"fragplus1\", \"cterm\"]:\n",
    "        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n",
    "            feature_names.append(\"_\".join([site, aa_property]))\n",
    "\n",
    "    feature_names.extend([\"length\", \"charge\"])\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a single peptide. Feel free to use your own name as a \"peptide\"; as\n",
    "long as it does not contain any non-amino acid characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_features = pd.DataFrame(encode_peptide(\"RALFGARIELS\", 2), columns=generate_feature_names())\n",
    "peptide_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Getting the target intensities\n",
    "\n",
    "The target intensities are the observed intensities which the model will learn to\n",
    "predict. Let's first try with a single spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_spectrum = train_val_spectra.iloc[4]\n",
    "\n",
    "peptide_targets =  pd.DataFrame({\n",
    "    \"b_target\": test_spectrum[\"parsed_intensity\"][\"b\"],\n",
    "    \"y_target\": test_spectrum[\"parsed_intensity\"][\"y\"],\n",
    "})\n",
    "peptide_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the intensities for the b- and y-ions, each ordered from 1 to 9. \n",
    "In MS²PIP, however, a clever trick is applied to reuse the computed features for each\n",
    "*fragment ion pair*. Doing so makes perfect sense, as both ions in such a fragment ion\n",
    "pair originated from the same fragmentation event. For this peptide, the fragment ion\n",
    "pairs are b1-y9, b2-y8, b3-y7, etc. To match all of the pairs, we simply have to reverse\n",
    "the y-ion series intensities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_targets =  pd.DataFrame({\n",
    "    \"b_target\": test_spectrum[\"parsed_intensity\"][\"b\"],\n",
    "    \"y_target\": test_spectrum[\"parsed_intensity\"][\"y\"][::-1],\n",
    "})\n",
    "peptide_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bringing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encode_peptide(test_spectrum[\"sequence\"], test_spectrum[\"charge\"])\n",
    "targets = np.stack([test_spectrum[\"parsed_intensity\"][\"b\"], test_spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n",
    "spectrum_id = np.full(shape=(targets.shape[0], 1), fill_value=test_spectrum[\"index\"])  # Repeat id for all ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack([spectrum_id, features, targets]), columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function applies these steps over a collection of spectra and returns\n",
    "the full feature/target table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ml_input(spectra):\n",
    "    tables = []\n",
    "    for spectrum in progress.track(spectra.to_dict(orient=\"records\")):\n",
    "        features = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\n",
    "        targets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n",
    "        spectrum_id = np.full(shape=(targets.shape[0], 1), fill_value=spectrum[\"index\"])  # Repeat id for all ions\n",
    "        table = np.hstack([spectrum_id, features, targets])\n",
    "        tables.append(table)\n",
    "\n",
    "    full_table = np.vstack(tables)\n",
    "    spectra_encoded = pd.DataFrame(full_table, columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\n",
    "    return spectra_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this might take some time, sometimes up to 30 minutes. To skip this step,\n",
    "simple download the file with pre-encoded features and targets, and load in two cells\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_encoded = generate_ml_input(train_val_spectra)\n",
    "train_val_encoded.to_feather(\"fragmentation-nist-humanhcd20160503-parsed-trainval-encoded.feather\")\n",
    "\n",
    "test_encoded = generate_ml_input(test_spectra)\n",
    "test_encoded.to_feather(\"fragmentation-nist-humanhcd20160503-parsed-test-encoded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this step to load pre-encoded features from a file:\n",
    "\n",
    "#train_val_encoded = pd.read_feather(\"ftp://ftp.pride.ebi.ac.uk/pub/databases/pride/resources/proteomicsml/fragmentation/nist-humanhcd20160503-parsed-trainval-encoded.feather\")\n",
    "#test_encoded = pd.read_feather(\"ftp://ftp.pride.ebi.ac.uk/pub/databases/pride/resources/proteomicsml/fragmentation/nist-humanhcd20160503-parsed-test-encoded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data we will use for training. Note that each spectrum comprises of\n",
    "multiple lines: One line per b/y-ion couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to train a simple model on the train set and evaluate its performance on the test\n",
    "set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  GradientBoostingRegressor()\n",
    "\n",
    "X_train = train_val_encoded.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_train = train_val_encoded[\"y_target\"]\n",
    "X_test = test_encoded.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_test = test_encoded[\"y_target\"]\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = reg.predict(X_test)\n",
    "np.corrcoef(y_test, y_test_pred)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible. Let's see if we can do better after hyperparameters optimization. For\n",
    "this, we can use the hyperopt package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(n_estimators):\n",
    "    # Define algorithm\n",
    "    reg =  GradientBoostingRegressor(n_estimators=n_estimators)\n",
    "\n",
    "    # Fit model\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    # Test model\n",
    "    y_test_pred = reg.predict(X_test)\n",
    "    correlation = np.corrcoef(y_test, y_test_pred)[0][1]\n",
    "\n",
    "    return {'loss': -correlation, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = fmin(\n",
    "  fn=objective,\n",
    "  space=10 + hp.randint('n_estimators', 980),\n",
    "  algo=tpe.suggest,\n",
    "  max_evals=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the default value of 100 estimators was used. According to\n",
    "this hyperopt run, using 912 estimators results in a more performant model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model again with this new hyperparameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  GradientBoostingRegressor(n_estimators=946)\n",
    "\n",
    "X_train = train_val_encoded.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_train = train_val_encoded[\"y_target\"]\n",
    "X_test = test_encoded.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_test = test_encoded[\"y_target\"]\n",
    "\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = reg.predict(X_test)\n",
    "np.corrcoef(y_test, y_test_pred)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better already. To get a more accurate view of the model performance, we should\n",
    "calculate the correlation per spectrum, instead of across the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_y = pd.DataFrame({\n",
    "    \"spectrum_id\": test_encoded[\"spectrum_id\"],\n",
    "    \"target_y\": y_test,\n",
    "    \"prediction_y\": y_test_pred,\n",
    "})\n",
    "prediction_df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_y = prediction_df_y.groupby(\"spectrum_id\").corr().iloc[::2]['prediction_y']\n",
    "corr_y.index = corr_y.index.droplevel(1)\n",
    "corr_y = corr_y.reset_index().rename(columns={\"prediction_y\": \"correlation\"})\n",
    "corr_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_y[\"correlation\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(\n",
    "    data=corr_y, x=\"correlation\",\n",
    "    fliersize=1,\n",
    "    kind=\"box\", aspect=4, height=2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! With some more hyperparameter optimization (optimizing only the number of trees\n",
    "is a bit crude) a lot more performance gains could be made. Take a look at the \n",
    "[Scikit Learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "to learn more about the various hyperparameters for the `GradientBoostingRegressor`.\n",
    "Alternatively, you could switch to the\n",
    "[XGBoost algorithm](https://xgboost.readthedocs.io/en/stable/), which is currently used\n",
    "by MS²PIP.\n",
    "\n",
    "And of course, this model can only predict y-ion intensities. You can repeat the\n",
    "training and optimization steps to train a model for b-ion intensities.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
