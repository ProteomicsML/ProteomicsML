{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NIST (part 2): Traditional ML: Gradient boosting\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Ralf Gabriels\n",
    "\n",
    "  orcid: 0000-0002-1679-1711\n",
    "\n",
    "  affiliations:\n",
    "    - VIB-UGent Center for Medical Biotechnology, VIB, Belgium\n",
    "    - Department of Biomolecular Medicine, Ghent University, Belgium\n",
    "\n",
    "---\n",
    "\n",
    "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/fragmentation/_nist-2-traditional-ml-gradient-boosting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "aa_properties = {\n",
    "    \"basicity\": np.array([37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104]),\n",
    "    \"helicity\": np.array([68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55]),\n",
    "    \"hydrophobicity\": np.array([51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70]),\n",
    "    \"pI\": np.array([32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28]),\n",
    "}\n",
    "\n",
    "properties_df = pd.DataFrame(aa_properties, index=amino_acids)\n",
    "properties_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peptide input\n",
    "# Feature engineering settings\n",
    "\n",
    "properties = np.array([\n",
    "    [37,35,59,129,94,0,210,81,191,81,106,101,117,115,343,49,90,60,134,104],  # basicity\n",
    "    [68,23,33,29,70,58,41,73,32,73,66,38,0,40,39,44,53,71,51,55],  # helicity\n",
    "    [51,75,25,35,100,16,3,94,0,94,82,12,0,22,22,21,39,80,98,70],  # hydrophobicity\n",
    "    [32,23,0,4,27,32,48,32,69,32,29,26,35,28,79,29,28,31,31,28],  # pI\n",
    "])\n",
    "\n",
    "quantiles = [0, 0.25, 0.5, 0.75, 1]\n",
    "aa_indices = {aa: i for i, aa in  enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "aa_to_index = np.vectorize(lambda aa: aa_indices[aa])\n",
    "\n",
    "def encode_peptide(sequence, charge):\n",
    "    # 4 properties * 5 quantiles * 3 ion types + 4 properties * 4 site + 2 global\n",
    "    n_features = 78\n",
    "    n_ions = len(sequence) - 1\n",
    "\n",
    "    # Encode amino acids as integers to index amino acid properties for peptide sequence\n",
    "    peptide_indexed = aa_to_index(np.array(list(sequence)))\n",
    "    peptide_properties = properties[:, peptide_indexed]\n",
    "\n",
    "    # Empty peptide_features array\n",
    "    peptide_features = np.full((n_ions, n_features), np.nan)\n",
    "\n",
    "    for b_ion_number in range(1, n_ions + 1):\n",
    "        # Calculate quantiles of features across peptide, b-ion, and y-ion\n",
    "        peptide_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties, quantiles, axis=1).transpose()\n",
    "        )\n",
    "        b_ion_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties[:,:b_ion_number], quantiles, axis=1).transpose()\n",
    "        )\n",
    "        y_ion_quantiles = np.hstack(\n",
    "            np.quantile(peptide_properties[:,b_ion_number:], quantiles, axis=1).transpose()\n",
    "        )\n",
    "\n",
    "        # Properties on specific sites: nterm, frag-1, frag+1, cterm\n",
    "        specific_site_indexes = np.array([0, b_ion_number - 1, b_ion_number, -1])\n",
    "        specific_site_properties = np.hstack(peptide_properties[:, specific_site_indexes].transpose())\n",
    "\n",
    "        # Global features: Length and charge\n",
    "        global_features = np.array([len(sequence), int(charge)])\n",
    "\n",
    "        # Assign to peptide_features array\n",
    "        peptide_features[b_ion_number - 1, 0:20] = peptide_quantiles\n",
    "        peptide_features[b_ion_number - 1, 20:40] = b_ion_quantiles\n",
    "        peptide_features[b_ion_number - 1, 40:60] = y_ion_quantiles\n",
    "        peptide_features[b_ion_number - 1, 60:76] = specific_site_properties\n",
    "        peptide_features[b_ion_number - 1, 76:78] = global_features\n",
    "\n",
    "    return peptide_features\n",
    "\n",
    "\n",
    "def generate_feature_names():\n",
    "    feature_names = []\n",
    "    for level in [\"peptide\", \"b\", \"y\"]:\n",
    "        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n",
    "            for quantile in [\"min\", \"q1\", \"q2\", \"q3\", \"max\"]:\n",
    "                feature_names.append(\"_\".join([level, aa_property, quantile]))\n",
    "    for site in [\"nterm\", \"fragmin1\", \"fragplus1\", \"cterm\"]:\n",
    "        for aa_property in [\"basicity\", \"helicity\", \"hydrophobicity\", \"pi\"]:\n",
    "            feature_names.append(\"_\".join([site, aa_property]))\n",
    "        \n",
    "    feature_names.extend([\"length\", \"charge\"])\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with a single peptide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_features = pd.DataFrame(encode_peptide(\"RALFGARIELS\", 2), columns=generate_feature_names())\n",
    "peptide_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Getting the target intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_targets =  pd.DataFrame({\n",
    "    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n",
    "    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"],\n",
    "})\n",
    "peptide_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_targets =  pd.DataFrame({\n",
    "    \"b_target\": spectrum[\"parsed_intensity\"][\"b\"],\n",
    "    \"y_target\": spectrum[\"parsed_intensity\"][\"y\"][::-1],\n",
    "})\n",
    "peptide_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\n",
    "targets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n",
    "spectrum_id = np.full(shape=(targets.shape[0], 1), fill_value=1, dtype=np.uint32)  # Repeat id for all ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.hstack([spectrum_id, features, targets]), columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `[::-1]` after `spectrum[\"parsed_intensity\"][\"y\"]`. Remember why we do this?\n",
    "\n",
    "Let's get a full feature/target table for all spectra in our dataset. Note that\n",
    "this might take some time, sometimes up to 30 minutes. To skip this step, simple\n",
    "download the file with pre-encoded features and targets, and load in two cells\n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "for i, spectrum in progress.track(enumerate(spectrum_list)):\n",
    "    features = encode_peptide(spectrum[\"sequence\"], spectrum[\"charge\"])\n",
    "    targets = np.stack([spectrum[\"parsed_intensity\"][\"b\"], spectrum[\"parsed_intensity\"][\"y\"][::-1]], axis=1)\n",
    "    spectrum_id = np.full(shape=(targets.shape[0], i), fill_value=1, dtype=np.uint32)  # Repeat id for all ions\n",
    "    table = np.hstack([spectrum_id, features, targets])\n",
    "    tables.append(table)\n",
    "\n",
    "full_table = np.vstack(tables)\n",
    "\n",
    "spectra_encoded = pd.DataFrame(full_table, columns=[\"spectrum_id\"] + generate_feature_names() + [\"b_target\",  \"y_target\"])\n",
    "spectra_encoded.to_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this step to load in pre-encoded features from a file:\n",
    "# spectra_encoded = pd.read_feather(\"human_hcd_tryp_best_spectra_encoded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data we will use for training. Note that each spectrum comprises of\n",
    "multiple lines: One line per b/y-ion couple. The only thing left to do is to\n",
    "split the data into train, validation, and test sets, according to the\n",
    "peptide-level split we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_encoded_trainval = spectra_encoded[spectra_encoded.index.isin(train_val_spectra.index)]\n",
    "spectra_encoded_test = spectra_encoded[spectra_encoded.index.isin(test_spectra.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Hyperparameter optimization and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg =  GradientBoostingRegressor()\n",
    "\n",
    "X_train = spectra_encoded_trainval.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_train = spectra_encoded_trainval[\"b_target\"]\n",
    "X_test = spectra_encoded_test.drop(columns=[\"spectrum_id\", \"b_target\",  \"y_target\"])\n",
    "y_test = spectra_encoded_test[\"b_target\"]\n",
    "\n",
    "reg.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(y_test, y_test_pred)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better by optimizing some hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(n_estimators):\n",
    "    # Define algorithm\n",
    "    reg =  GradientBoostingRegressor(n_estimators=n_estimators)\n",
    "\n",
    "    # Fit model\n",
    "    reg.fit(X_test, y_test)\n",
    "\n",
    "    # Test model\n",
    "    y_test_pred = reg.predict(X_test)\n",
    "    correlation = np.corrcoef(y_test, y_test_pred)[0][1]\n",
    "    \n",
    "    return {'loss': -correlation, 'status': STATUS_OK}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = fmin(\n",
    "  fn=objective,\n",
    "  space=hp.randint('n_estimators', 10, 1000),\n",
    "  algo=tpe.suggest,\n",
    "  max_evals=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Initially, the default value of 100 estimators was used. According to\n",
    "this hyperopt run, using 874 estimators results in a more performant model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c339fba8800f289223ed8cfd6e3b81401a206d437a496d69810d81cfdc690c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
