{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGofq6EU6DnT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "title: \"Prosit-style GRU with pre-annotated ProteomeTools data\"\n",
    "\n",
    "date: last-modified\n",
    "\n",
    "author:\n",
    "\n",
    "- name: Siegfried Gessulat\n",
    "\n",
    "  orcid: 0000-0001-5530-0674\n",
    "  \n",
    "  affiliations:\n",
    "    - MSAID, Gmbh, Berlin, Germany\n",
    "---\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ProteomicsML/ProteomicsML/blob/main/tutorials/fragmentation/_proteometools-prosit.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDgGzv2F6Dna",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Download and prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOxZuicHUnuA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load ProteomeTools data from Figshare\n",
    "!wget https://figshare.com/ndownloader/files/12506534 \n",
    "!mv 12506534 prosit_2018_holdout.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "io--2mqTVHJN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "\n",
    "# Using the alphabet as defined in Prosit: \n",
    "# https://github.com/kusterlab/prosit/blob/master/prosit/constants.py#L21-L43\n",
    "PROSIT_ALHABET = {\n",
    "    \"A\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"K\": 9,\n",
    "    \"L\": 10,\n",
    "    \"M\": 11,\n",
    "    \"N\": 12,\n",
    "    \"P\": 13,\n",
    "    \"Q\": 14,\n",
    "    \"R\": 15,\n",
    "    \"S\": 16,\n",
    "    \"T\": 17,\n",
    "    \"V\": 18,\n",
    "    \"W\": 19,\n",
    "    \"Y\": 20,\n",
    "    \"M(ox)\": 21,\n",
    "}\n",
    "PROSIT_INDEXED_ALPHABET = {i: c for c, i in PROSIT_ALHABET.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6Fi6tembJj1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the downloaded data to a dataframe\n",
    "with h5.File('prosit_2018_holdout.hdf5', 'r') as f:\n",
    "  KEY_ARRAY = [\"sequence_integer\", \"precursor_charge_onehot\", \"intensities_raw\"]\n",
    "  KEY_SCALAR = [\"collision_energy_aligned_normed\", \"collision_energy\"]\n",
    "  df = pd.DataFrame({key: list(f[key][...]) for key in KEY_ARRAY})\n",
    "  for key in KEY_SCALAR:\n",
    "    df[key] = f[key][...]\n",
    "\n",
    "# Add convenience columns\n",
    "df['precursor_charge'] = df.precursor_charge_onehot.map(lambda a: a.argmax() + 1)\n",
    "df['sequence_maxquant'] = df.sequence_integer.map(lambda s: \"\".join(PROSIT_INDEXED_ALPHABET[i] for i in s if i != 0))\n",
    "df['sequence_length'] = df.sequence_integer.map(lambda s: np.count_nonzero(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNtulaYz-_WE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Inspecting data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGYexvXnXA_P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['precursor_charge'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsAu7wW3p3j6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['collision_energy'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YH2rgt4p3ps",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['sequence_length'].hist(bins=30-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ycIPIVvEbGo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZBC05d9cbJC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def split_dataframe(df, \n",
    "                    unique_column, \n",
    "                    ratio_training=0.8, \n",
    "                    ratio_validation=0.1, \n",
    "                    ratio_test=0.1):\n",
    "  \"\"\"\n",
    "  This function splits the dataframe in three splits and makes sure that values\n",
    "  of `unique_column` are unique to each of the splits. This is helpful if, for \n",
    "  example, you have non-unique sequence in `unique_column` but want to ensure \n",
    "  that a sequence value is unique to one of the splits.\n",
    "  \"\"\"\n",
    "\n",
    "  assert ratio_training + ratio_validation + ratio_test == 1\n",
    "\n",
    "  unique = list(set(df[unique_column]))\n",
    "  n_unique = len(unique)\n",
    "  shuffle(unique)\n",
    "\n",
    "  train_split = int(n_unique * ratio_training)\n",
    "  val_split = int(n_unique * (ratio_training + ratio_validation))\n",
    "\n",
    "  unique_train = unique[:train_split]\n",
    "  unique_validation = unique[train_split:val_split]\n",
    "  unique_test = unique[val_split:]\n",
    "\n",
    "  assert len(unique_train) + len(unique_validation) + len(unique_test) == n_unique\n",
    "\n",
    "  df_train = df[df[unique_column].isin(unique_train)]\n",
    "  df_validation = df[df[unique_column].isin(unique_validation)]\n",
    "  df_test = df[df[unique_column].isin(unique_test)]\n",
    "\n",
    "  assert len(df_train) + len(df_validation) + len(df_test) == len(df)\n",
    "\n",
    "  return df_train, df_validation, df_test\n",
    "\n",
    "df_train, df_validation, df_test = split_dataframe(df, unique_column='sequence_maxquant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xgc2oDGbyLeZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "INPUT_COLUMNS = ('sequence_integer', 'precursor_charge_onehot', 'collision_energy_aligned_normed')\n",
    "OUTPUT_COLUMN = 'intensities_raw'\n",
    "\n",
    "x_train = [np.vstack(df_train[column]) for column in INPUT_COLUMNS]\n",
    "y_train = np.vstack(df_train[OUTPUT_COLUMN])\n",
    "\n",
    "x_validation = [np.vstack(df_validation[column]) for column in INPUT_COLUMNS]\n",
    "y_validation = np.vstack(df_validation[OUTPUT_COLUMN])\n",
    "\n",
    "x_test = [np.vstack(df_test[column]) for column in INPUT_COLUMNS]\n",
    "y_test = np.vstack(df_test[OUTPUT_COLUMN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5-yfe7yEfTv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSvDvXSYqDYG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup model and trainig parameters\n",
    "DIM_LATENT = 124\n",
    "DIM_EMBEDDING_IN = max(PROSIT_ALHABET.values()) + 1  # max value + zero for padding\n",
    "DIM_EMBEDDING_OUT = 32\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCOIAXMEyPl9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "# Build the model with input layers for sequence, precursor charge, and collision energy\n",
    "in_sequence = Input(shape=[x_train[0].shape[1]], name=\"in_sequence\")\n",
    "in_precursor_charge = Input(shape=[x_train[1].shape[1]], name=\"in_precursor_charge\")\n",
    "in_collision_energy = Input(shape=[x_train[2].shape[1]], name=\"in_collision_energy\")\n",
    "\n",
    "x_s = Embedding(input_dim=DIM_EMBEDDING_IN, output_dim=DIM_EMBEDDING_OUT)(in_sequence)\n",
    "x_s = GRU(DIM_LATENT)(x_s)\n",
    "x_z = Dense(DIM_LATENT)(in_precursor_charge)\n",
    "x_e = Dense(DIM_LATENT)(in_collision_energy)\n",
    "x = Multiply()([x_s, x_z, x_e])\n",
    "out_intensities = Dense(y_train.shape[1])(x)\n",
    "\n",
    "model = Model([in_sequence, in_precursor_charge, in_collision_energy], out_intensities)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81QzXl7AyUX7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def masked_spectral_distance(true, pred):\n",
    "    # This is the spectral angle implementation as used in Prosit\n",
    "    # See https://github.com/kusterlab/prosit/blob/master/prosit/losses.py#L4-L16\n",
    "    # Note, fragment ions that cannot exists (i.e. y20 for a 7mer) must have the value  -1.\n",
    "    import keras.backend as k\n",
    "\n",
    "    epsilon = k.epsilon()\n",
    "    pred_masked = ((true + 1) * pred) / (true + 1 + epsilon)\n",
    "    true_masked = ((true + 1) * true) / (true + 1 + epsilon)\n",
    "    pred_norm = k.l2_normalize(true_masked, axis=-1)\n",
    "    true_norm = k.l2_normalize(pred_masked, axis=-1)\n",
    "    product = k.sum(pred_norm * true_norm, axis=1)\n",
    "    arccos = tf.acos(product)\n",
    "    return 2 * arccos / np.pi\n",
    "\n",
    "model.compile(optimizer='Adam', loss=masked_spectral_distance)\n",
    "history = model.fit(x=x_train, y=y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EzD4SubEn07",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xZm7KL56n7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting the training history \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(EPOCHS), history.history['loss'], '-', color='r', label='Training loss')\n",
    "plt.plot(range(EPOCHS), history.history['val_loss'], '--', color='r', label='Validation loss')\n",
    "plt.title(f'Training and validation loss across epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zi67m4cDLDc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_spectral_angle = model.evaluate(x_test, y_test)\n",
    "test_spectral_angle"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c339fba8800f289223ed8cfd6e3b81401a206d437a496d69810d81cfdc690c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}